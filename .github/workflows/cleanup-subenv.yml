name: Cleanup Sub-Environment

on:
  workflow_dispatch:
    inputs:
      environment:
        description: "Sub-environment to clean up (e.g., dev01, qa02, staging03)"
        required: true
        type: string
      force_cleanup:
        description: "Force cleanup without confirmation"
        required: false
        default: false
        type: boolean
      dry_run:
        description: "Perform a dry run (show what would be deleted)"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  contents: read

env:
  AWS_REGION: us-west-1

jobs:
  cleanup:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Validate Environment
        run: |
          ENVIRONMENT="${{ github.event.inputs.environment }}"

          # Validate environment name format
          if [[ ! "$ENVIRONMENT" =~ ^(dev|qa|staging|hotfix)[0-9][0-9]$ ]]; then
            echo "‚ùå Invalid environment format: $ENVIRONMENT"
            echo "Valid formats: dev01-dev99, qa01-qa99, staging01-staging99, hotfix01-hotfix99"
            exit 1
          fi

          # Extract main environment
          MAIN_ENV=$(echo "$ENVIRONMENT" | sed 's/[0-9]*$//')

          echo "‚úÖ Environment validation passed"
          echo "Sub-environment: $ENVIRONMENT"
          echo "Main environment: $MAIN_ENV"

          echo "ENVIRONMENT=$ENVIRONMENT" >> $GITHUB_ENV
          echo "MAIN_ENV=$MAIN_ENV" >> $GITHUB_ENV

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Terraform Destroy
        working-directory: terraform/environments/${{ env.MAIN_ENV }}
        run: |
          echo "üßπ Destroying infrastructure for ${{ env.ENVIRONMENT }}..."

          # Initialize Terraform
          terraform init

          # Create tfvars file
          cat > environment.auto.tfvars << EOF
          environment = "${{ env.MAIN_ENV }}"
          sub_environment = "${{ env.ENVIRONMENT }}"
          project_name = "dpp"
          domain_name = "hibiji.com"
          aws_region = "${{ env.AWS_REGION }}"
          alert_emails = ["alerts@maras.co"]
          EOF

          if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
            echo "üîç DRY RUN: Showing what would be destroyed..."
            terraform plan -destroy -var-file="environment.auto.tfvars"
          else
            echo "üö® DESTROYING infrastructure..."
            
            if [ "${{ github.event.inputs.force_cleanup }}" = "true" ]; then
              terraform destroy -auto-approve -var-file="environment.auto.tfvars"
            else
              echo "‚ùå Manual approval required. Set force_cleanup=true to proceed."
              exit 1
            fi
          fi

      - name: Clean up orphaned AWS resources
        if: always() && github.event.inputs.dry_run != 'true' && github.event.inputs.force_cleanup == 'true'
        # Run comprehensive cleanup even if Terraform destroy fails
        # This handles AWS Batch dependencies properly
        run: |
          # Install jq for JSON processing
          sudo apt-get update && sudo apt-get install -y jq

          echo "üßπ Cleaning up orphaned AWS resources for ${{ env.ENVIRONMENT }}..."

          ENVIRONMENT="${{ env.ENVIRONMENT }}"
          MAIN_ENV="${{ env.MAIN_ENV }}"
          PROJECT_NAME="dpp"

          # Clean up Lambda functions
          echo "üßπ Cleaning up Lambda functions..."
          LAMBDA_FUNCTIONS=$(aws lambda list-functions \
            --query "Functions[?starts_with(FunctionName, '${MAIN_ENV}-${ENVIRONMENT}')].FunctionName" \
            --output text)

          for func in $LAMBDA_FUNCTIONS; do
            if [ -n "$func" ] && [ "$func" != "None" ]; then
              echo "üóëÔ∏è Deleting Lambda function: $func"
              aws lambda delete-function --function-name "$func" || echo "‚ö†Ô∏è Failed to delete Lambda function: $func"
            fi
          done

          # Clean up API Gateway v2 APIs
          echo "üßπ Cleaning up API Gateway v2 APIs..."
          API_IDS=$(aws apigatewayv2 get-apis \
            --query "Items[?starts_with(Name, '${MAIN_ENV}-${ENVIRONMENT}')].ApiId" \
            --output text)

          for api_id in $API_IDS; do
            if [ -n "$api_id" ] && [ "$api_id" != "None" ]; then
              echo "üóëÔ∏è Deleting API Gateway: $api_id"
              aws apigatewayv2 delete-api --api-id "$api_id" || echo "‚ö†Ô∏è Failed to delete API Gateway: $api_id"
            fi
          done

          # Clean up S3 buckets
          echo "üßπ Cleaning up S3 buckets..."
          S3_BUCKETS=$(aws s3api list-buckets \
            --query "Buckets[?starts_with(Name, '${MAIN_ENV}-${ENVIRONMENT}')].Name" \
            --output text)

          for bucket in $S3_BUCKETS; do
            if [ -n "$bucket" ] && [ "$bucket" != "None" ]; then
              echo "üóëÔ∏è Emptying and deleting S3 bucket: $bucket"
              aws s3 rm s3://$bucket --recursive || echo "‚ö†Ô∏è Failed to empty bucket: $bucket"
              aws s3api delete-bucket --bucket "$bucket" || echo "‚ö†Ô∏è Failed to delete bucket: $bucket"
            fi
          done

          # Clean up CloudWatch Log Groups
          echo "üßπ Cleaning up CloudWatch Log Groups..."
          LOG_GROUPS=$(aws logs describe-log-groups \
            --query "logGroups[?contains(logGroupName, '${ENVIRONMENT}')].logGroupName" \
            --output text)

          for log_group in $LOG_GROUPS; do
            if [ -n "$log_group" ] && [ "$log_group" != "None" ]; then
              echo "üóëÔ∏è Deleting log group: $log_group"
              aws logs delete-log-group --log-group-name "$log_group" || echo "‚ö†Ô∏è Failed to delete log group: $log_group"
            fi
          done

          # Clean up AWS Batch resources (proper order: Job Definitions ‚Üí Job Queues ‚Üí Compute Environments)
          echo "üßπ Cleaning up AWS Batch Job Definitions..."
          JOB_DEFINITIONS=$(aws batch describe-job-definitions \
            --query "jobDefinitions[?starts_with(jobDefinitionName, '${MAIN_ENV}-${ENVIRONMENT}')].jobDefinitionArn" \
            --output text)

          for job_def in $JOB_DEFINITIONS; do
            if [ -n "$job_def" ] && [ "$job_def" != "None" ]; then
              echo "üóëÔ∏è Deregistering Batch job definition: $job_def"
              aws batch deregister-job-definition --job-definition "$job_def" || echo "‚ö†Ô∏è Failed to deregister job definition: $job_def"
            fi
          done

          echo "üßπ Cleaning up AWS Batch Job Queues..."
          JOB_QUEUES=$(aws batch describe-job-queues \
            --query "jobQueues[?starts_with(jobQueueName, '${MAIN_ENV}-${ENVIRONMENT}')].jobQueueName" \
            --output text)

          for queue in $JOB_QUEUES; do
            if [ -n "$queue" ] && [ "$queue" != "None" ]; then
              echo "üîÑ Disabling Batch job queue: $queue"
              aws batch update-job-queue --job-queue "$queue" --state DISABLED || echo "‚ö†Ô∏è Failed to disable job queue: $queue"
              
              echo "‚è≥ Waiting for job queue to be disabled..."
              sleep 30
              
              echo "üóëÔ∏è Deleting Batch job queue: $queue"
              aws batch delete-job-queue --job-queue "$queue" || echo "‚ö†Ô∏è Failed to delete job queue: $queue"
            fi
          done

          echo "üßπ Cleaning up AWS Batch Compute Environments..."
          COMPUTE_ENVS=$(aws batch describe-compute-environments \
            --query "computeEnvironments[?starts_with(computeEnvironmentName, '${MAIN_ENV}-${ENVIRONMENT}')].computeEnvironmentName" \
            --output text)

          for env in $COMPUTE_ENVS; do
            if [ -n "$env" ] && [ "$env" != "None" ]; then
              echo "üîÑ Disabling Batch compute environment: $env"
              aws batch update-compute-environment --compute-environment "$env" --state DISABLED || echo "‚ö†Ô∏è Failed to disable compute environment: $env"
              
              echo "‚è≥ Waiting for compute environment to be disabled..."
              sleep 60
              
              echo "üóëÔ∏è Deleting Batch compute environment: $env"
              aws batch delete-compute-environment --compute-environment "$env" || echo "‚ö†Ô∏è Failed to delete compute environment: $env"
            fi
          done

          # Clean up SQS Queues (including DLQ)
          echo "üßπ Cleaning up SQS Queues..."
          SQS_QUEUES=$(aws sqs list-queues \
            --queue-name-prefix "${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}" \
            --query "QueueUrls" --output text)

          for queue_url in $SQS_QUEUES; do
            if [ -n "$queue_url" ] && [ "$queue_url" != "None" ]; then
              echo "üóëÔ∏è Deleting SQS queue: $queue_url"
              aws sqs delete-queue --queue-url "$queue_url" || echo "‚ö†Ô∏è Failed to delete SQS queue: $queue_url"
            fi
          done

          # Clean up ECR Repositories  
          echo "üßπ Cleaning up ECR Repositories..."
          ECR_REPOS=$(aws ecr describe-repositories \
            --query "repositories[?starts_with(repositoryName, '${MAIN_ENV}-${ENVIRONMENT}')].repositoryName" \
            --output text)

          for repo in $ECR_REPOS; do
            if [ -n "$repo" ] && [ "$repo" != "None" ]; then
              echo "üóëÔ∏è Emptying ECR repository: $repo"
              # Delete all images first
              IMAGES=$(aws ecr list-images --repository-name "$repo" --query 'imageIds[*]' --output json)
              if [ "$IMAGES" != "[]" ] && [ -n "$IMAGES" ]; then
                aws ecr batch-delete-image --repository-name "$repo" --image-ids "$IMAGES" || echo "‚ö†Ô∏è Failed to delete images from: $repo"
              fi
              
              echo "üóëÔ∏è Deleting ECR repository: $repo"
              aws ecr delete-repository --repository-name "$repo" || echo "‚ö†Ô∏è Failed to delete ECR repository: $repo"
            fi
          done

          # Clean up RDS Proxy endpoints
          echo "üßπ Cleaning up RDS Proxy endpoints..."
          RDS_PROXIES=$(aws rds describe-db-proxies \
            --query "DBProxies[?starts_with(DBProxyName, '${MAIN_ENV}-${ENVIRONMENT}')].DBProxyName" \
            --output text)

          for proxy in $RDS_PROXIES; do
            if [ -n "$proxy" ] && [ "$proxy" != "None" ]; then
              echo "üóëÔ∏è Deleting RDS Proxy: $proxy"
              aws rds delete-db-proxy --db-proxy-name "$proxy" || echo "‚ö†Ô∏è Failed to delete RDS proxy: $proxy"
            fi
          done

          # Clean up Secrets Manager secrets
          echo "üßπ Cleaning up Secrets Manager secrets..."
          SECRETS=$(aws secretsmanager list-secrets \
            --query "SecretList[?starts_with(Name, '${MAIN_ENV}-${ENVIRONMENT}')].Name" \
            --output text)

          for secret in $SECRETS; do
            if [ -n "$secret" ] && [ "$secret" != "None" ]; then
              echo "üóëÔ∏è Deleting secret: $secret"
              aws secretsmanager delete-secret --secret-id "$secret" --force-delete-without-recovery || echo "‚ö†Ô∏è Failed to delete secret: $secret"
            fi
          done

          # Clean up RDS instances and clusters
          echo "üßπ Cleaning up RDS resources..."
          RDS_CLUSTERS=$(aws rds describe-db-clusters \
            --query "DBClusters[?starts_with(DBClusterIdentifier, '${ENVIRONMENT}')].DBClusterIdentifier" \
            --output text)

          for cluster in $RDS_CLUSTERS; do
            if [ -n "$cluster" ] && [ "$cluster" != "None" ]; then
              echo "üóëÔ∏è Deleting RDS cluster: $cluster"
              # Delete cluster instances first
              CLUSTER_INSTANCES=$(aws rds describe-db-clusters \
                --db-cluster-identifier "$cluster" \
                --query "DBClusters[0].DBClusterMembers[*].DBInstanceIdentifier" \
                --output text)
              
              for instance in $CLUSTER_INSTANCES; do
                if [ -n "$instance" ] && [ "$instance" != "None" ]; then
                  echo "üóëÔ∏è Deleting RDS instance: $instance"
                  aws rds delete-db-instance --db-instance-identifier "$instance" --skip-final-snapshot || echo "‚ö†Ô∏è Failed to delete RDS instance: $instance"
                fi
              done
              
              # Wait for instances to be deleted before deleting cluster
              echo "‚è≥ Waiting for instances to be deleted..."
              sleep 60
              
              aws rds delete-db-cluster --db-cluster-identifier "$cluster" --skip-final-snapshot || echo "‚ö†Ô∏è Failed to delete RDS cluster: $cluster"
            fi
          done

          # Clean up CloudFront distributions
          echo "üßπ Cleaning up CloudFront distributions..."
          DISTRIBUTIONS=$(aws cloudfront list-distributions \
            --query "DistributionList.Items[?contains(Aliases.Items || [''], '${ENVIRONMENT}')].{Id:Id,DomainName:DomainName}" \
            --output text)

          if [ -n "$DISTRIBUTIONS" ] && [ "$DISTRIBUTIONS" != "None" ]; then
            echo "$DISTRIBUTIONS" | while read id domain; do
              if [ -n "$id" ] && [ "$id" != "None" ]; then
                echo "üóëÔ∏è Disabling CloudFront distribution: $id ($domain)"
                
                # Get current distribution config
                aws cloudfront get-distribution-config --id "$id" --query 'DistributionConfig' > dist_config.json
                ETAG=$(aws cloudfront get-distribution-config --id "$id" --query 'ETag' --output text)
                
                # Disable distribution
                jq '.Enabled = false' dist_config.json > dist_config_disabled.json
                aws cloudfront update-distribution --id "$id" --distribution-config file://dist_config_disabled.json --if-match "$ETAG" || echo "‚ö†Ô∏è Failed to disable distribution: $id"
                
                echo "‚è≥ Distribution disabled. Manual deletion required after propagation."
              fi
            done
          fi

          # Verification: Check for any remaining AWS Batch resources
          echo "üîç Verifying AWS Batch cleanup..."
          REMAINING_QUEUES=$(aws batch describe-job-queues \
            --query "jobQueues[?starts_with(jobQueueName, '${MAIN_ENV}-${ENVIRONMENT}')].jobQueueName" \
            --output text)
          REMAINING_COMPUTE_ENVS=$(aws batch describe-compute-environments \
            --query "computeEnvironments[?starts_with(computeEnvironmentName, '${MAIN_ENV}-${ENVIRONMENT}')].computeEnvironmentName" \
            --output text)

          if [ -n "$REMAINING_QUEUES" ] && [ "$REMAINING_QUEUES" != "None" ]; then
            echo "‚ö†Ô∏è Warning: Some Batch job queues may still be deleting: $REMAINING_QUEUES"
            echo "üí° These should finish deleting automatically within a few minutes"
          fi

          if [ -n "$REMAINING_COMPUTE_ENVS" ] && [ "$REMAINING_COMPUTE_ENVS" != "None" ]; then
            echo "‚ö†Ô∏è Warning: Some Batch compute environments may still be deleting: $REMAINING_COMPUTE_ENVS"
            echo "üí° These should finish deleting automatically within a few minutes"
          fi

          # Clean up Route53 DNS Records  
          echo "üßπ Cleaning up Route53 DNS Records..."
          WEBSITE_RECORD="${ENVIRONMENT}.hibiji.com"
          API_RECORD="${ENVIRONMENT}-api.hibiji.com"

          # Get the hosted zone ID for hibiji.com
          HOSTED_ZONE_ID=$(aws route53 list-hosted-zones \
            --query "HostedZones[?Name=='hibiji.com.'].Id" \
            --output text | sed 's|/hostedzone/||')

          if [ -n "$HOSTED_ZONE_ID" ] && [ "$HOSTED_ZONE_ID" != "None" ]; then
            # Delete website DNS record
            if aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
               --query "ResourceRecordSets[?Name=='${WEBSITE_RECORD}.']" | grep -q "CNAME"; then
              echo "üóëÔ∏è Deleting Route53 record: $WEBSITE_RECORD"
              aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
                --change-batch "{
                  \"Changes\": [{
                    \"Action\": \"DELETE\",
                    \"ResourceRecordSet\": $(aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
                      --query "ResourceRecordSets[?Name=='${WEBSITE_RECORD}.']" --output json | jq '.[0]')
                  }]
                }" || echo "‚ö†Ô∏è Failed to delete website record: $WEBSITE_RECORD"
            fi
            
            # Delete API DNS record
            if aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
               --query "ResourceRecordSets[?Name=='${API_RECORD}.']" | grep -q "CNAME"; then
              echo "üóëÔ∏è Deleting Route53 record: $API_RECORD"
              aws route53 change-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
                --change-batch "{
                  \"Changes\": [{
                    \"Action\": \"DELETE\",
                    \"ResourceRecordSet\": $(aws route53 list-resource-record-sets --hosted-zone-id "$HOSTED_ZONE_ID" \
                      --query "ResourceRecordSets[?Name=='${API_RECORD}.']" --output json | jq '.[0]')
                  }]
                }" || echo "‚ö†Ô∏è Failed to delete API record: $API_RECORD"
            fi
          fi

          # Clean up DB Subnet Groups
          echo "üßπ Cleaning up DB Subnet Groups..."
          DB_SUBNET_GROUPS=$(aws rds describe-db-subnet-groups \
            --query "DBSubnetGroups[?starts_with(DBSubnetGroupName, '${MAIN_ENV}-${ENVIRONMENT}')].DBSubnetGroupName" \
            --output text)

          for subnet_group in $DB_SUBNET_GROUPS; do
            if [ -n "$subnet_group" ] && [ "$subnet_group" != "None" ]; then
              echo "üóëÔ∏è Deleting DB subnet group: $subnet_group"
              aws rds delete-db-subnet-group --db-subnet-group-name "$subnet_group" || echo "‚ö†Ô∏è Failed to delete DB subnet group: $subnet_group"
            fi
          done

          # Clean up Security Groups (after other resources)
          echo "üßπ Cleaning up Security Groups..."
          SECURITY_GROUPS=$(aws ec2 describe-security-groups \
            --filters "Name=group-name,Values=${MAIN_ENV}-${ENVIRONMENT}-*" \
            --query "SecurityGroups[].GroupId" --output text)

          for sg_id in $SECURITY_GROUPS; do
            if [ -n "$sg_id" ] && [ "$sg_id" != "None" ]; then
              echo "üóëÔ∏è Deleting security group: $sg_id"
              aws ec2 delete-security-group --group-id "$sg_id" || echo "‚ö†Ô∏è Failed to delete security group: $sg_id (may have dependencies)"
            fi
          done

          # Clean up Subnets
          echo "üßπ Cleaning up Subnets..."
          VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-vpc" \
            --query "Vpcs[0].VpcId" --output text)

          if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
            SUBNETS=$(aws ec2 describe-subnets \
              --filters "Name=vpc-id,Values=$VPC_ID" \
              --query "Subnets[].SubnetId" --output text)

            for subnet_id in $SUBNETS; do
              if [ -n "$subnet_id" ] && [ "$subnet_id" != "None" ]; then
                echo "üóëÔ∏è Deleting subnet: $subnet_id"
                aws ec2 delete-subnet --subnet-id "$subnet_id" || echo "‚ö†Ô∏è Failed to delete subnet: $subnet_id"
              fi
            done

            # Clean up Internet Gateway
            echo "üßπ Cleaning up Internet Gateway..."
            IGW_ID=$(aws ec2 describe-internet-gateways \
              --filters "Name=attachment.vpc-id,Values=$VPC_ID" \
              --query "InternetGateways[0].InternetGatewayId" --output text)

            if [ -n "$IGW_ID" ] && [ "$IGW_ID" != "None" ]; then
              echo "üóëÔ∏è Detaching and deleting Internet Gateway: $IGW_ID"
              aws ec2 detach-internet-gateway --internet-gateway-id "$IGW_ID" --vpc-id "$VPC_ID" || echo "‚ö†Ô∏è Failed to detach IGW"
              aws ec2 delete-internet-gateway --internet-gateway-id "$IGW_ID" || echo "‚ö†Ô∏è Failed to delete IGW: $IGW_ID"
            fi

            # Clean up VPC (last)
            echo "üóëÔ∏è Deleting VPC: $VPC_ID"
            aws ec2 delete-vpc --vpc-id "$VPC_ID" || echo "‚ö†Ô∏è Failed to delete VPC: $VPC_ID"
          fi

          # Clean up IAM Roles (environment-specific)
          echo "üßπ Cleaning up IAM Roles..."
          IAM_ROLES=$(aws iam list-roles \
            --query "Roles[?starts_with(RoleName, '${MAIN_ENV}-${ENVIRONMENT}')].RoleName" \
            --output text)

          for role in $IAM_ROLES; do
            if [ -n "$role" ] && [ "$role" != "None" ]; then
              echo "üóëÔ∏è Cleaning up IAM role: $role"
              
              # Detach policies first
              ATTACHED_POLICIES=$(aws iam list-attached-role-policies --role-name "$role" \
                --query "AttachedPolicies[].PolicyArn" --output text)
              for policy_arn in $ATTACHED_POLICIES; do
                if [ -n "$policy_arn" ] && [ "$policy_arn" != "None" ]; then
                  aws iam detach-role-policy --role-name "$role" --policy-arn "$policy_arn" || echo "‚ö†Ô∏è Failed to detach policy: $policy_arn"
                fi
              done
              
              # Delete inline policies
              INLINE_POLICIES=$(aws iam list-role-policies --role-name "$role" \
                --query "PolicyNames" --output text)
              for policy_name in $INLINE_POLICIES; do
                if [ -n "$policy_name" ] && [ "$policy_name" != "None" ]; then
                  aws iam delete-role-policy --role-name "$role" --policy-name "$policy_name" || echo "‚ö†Ô∏è Failed to delete inline policy: $policy_name"
                fi
              done
              
              # Delete instance profiles
              INSTANCE_PROFILES=$(aws iam list-instance-profiles-for-role --role-name "$role" \
                --query "InstanceProfiles[].InstanceProfileName" --output text)
              for profile_name in $INSTANCE_PROFILES; do
                if [ -n "$profile_name" ] && [ "$profile_name" != "None" ]; then
                  aws iam remove-role-from-instance-profile --instance-profile-name "$profile_name" --role-name "$role" || echo "‚ö†Ô∏è Failed to remove role from instance profile"
                  aws iam delete-instance-profile --instance-profile-name "$profile_name" || echo "‚ö†Ô∏è Failed to delete instance profile: $profile_name"
                fi
              done
              
              # Delete the role
              aws iam delete-role --role-name "$role" || echo "‚ö†Ô∏è Failed to delete IAM role: $role"
            fi
          done

          # Clean up Network Interfaces (ENIs)
          echo "üßπ Cleaning up Network Interfaces..."
          ENI_IDS=$(aws ec2 describe-network-interfaces \
            --filters "Name=description,Values=*${ENVIRONMENT}*" \
            --query "NetworkInterfaces[?State=='available'].NetworkInterfaceId" \
            --output text)

          for eni_id in $ENI_IDS; do
            if [ -n "$eni_id" ] && [ "$eni_id" != "None" ]; then
              echo "üóëÔ∏è Deleting network interface: $eni_id"
              aws ec2 delete-network-interface --network-interface-id "$eni_id" || echo "‚ö†Ô∏è Failed to delete ENI: $eni_id"
            fi
          done

          echo "‚úÖ Comprehensive resource cleanup completed!"

      - name: Cleanup Summary
        if: always()
        run: |
          echo "üéØ Cleanup Summary"
          echo "=================="
          echo "Environment: ${{ env.ENVIRONMENT }}"
          echo "Main Environment: ${{ env.MAIN_ENV }}"
          echo "Dry Run: ${{ github.event.inputs.dry_run }}"
          echo "Force Cleanup: ${{ github.event.inputs.force_cleanup }}"
          echo ""

          if [ "${{ github.event.inputs.dry_run }}" = "true" ]; then
            echo "‚úÖ Dry run completed - no resources were deleted"
          elif [ "${{ github.event.inputs.force_cleanup }}" = "true" ]; then
            echo "‚úÖ Cleanup completed for environment: ${{ env.ENVIRONMENT }}"
            echo ""
            echo "üìã Resources cleaned up:"
            echo "  ‚Ä¢ Terraform-managed infrastructure"
            echo "  ‚Ä¢ Route53 DNS records (${ENVIRONMENT}.hibiji.com, ${ENVIRONMENT}-api.hibiji.com)"
            echo "  ‚Ä¢ VPC infrastructure (VPC, subnets, internet gateways)"
            echo "  ‚Ä¢ Security groups (database and lambda security groups)"
            echo "  ‚Ä¢ DB subnet groups"
            echo "  ‚Ä¢ IAM roles, policies, and instance profiles" 
            echo "  ‚Ä¢ Network interfaces (ENIs)"
            echo "  ‚Ä¢ AWS Batch job definitions, queues, and compute environments"
            echo "  ‚Ä¢ SQS queues (main queue + dead letter queue)"
            echo "  ‚Ä¢ ECR repositories (ML service container images)"
            echo "  ‚Ä¢ RDS Proxy endpoints"
            echo "  ‚Ä¢ Lambda functions"
            echo "  ‚Ä¢ API Gateway v2 APIs"
            echo "  ‚Ä¢ S3 buckets"
            echo "  ‚Ä¢ CloudWatch Log Groups"
            echo "  ‚Ä¢ Secrets Manager secrets"
            echo "  ‚Ä¢ RDS clusters and instances"
            echo "  ‚Ä¢ CloudFront distributions (disabled)"
            echo ""
            echo "‚ö†Ô∏è Note: CloudFront distributions require manual deletion after propagation"
          else
            echo "‚ùå Cleanup was not performed (force_cleanup not enabled)"
          fi
