name: Digital Persona Platform - Unified CI/CD (Optimized v2)

on:
  push:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  pull_request:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  workflow_dispatch:
    inputs:
      force_deploy:
        description: "Force deployment even if tests fail"
        required: false
        default: false
        type: boolean
      skip_tests:
        description: "Skip test execution for emergency deployments"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  security-events: write
  actions: read
  contents: read

env:
  AWS_REGION: us-west-1
  PYTHON_VERSION: "3.11"
  NODE_VERSION: "18"
  DOMAIN: hibiji.com
  TERRAFORM_VERSION: "1.5.0"

# Prevent multiple runs from conflicting
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===========================================
  # ENVIRONMENT DETECTION
  # ===========================================
  detect-environment:
    name: ðŸ” Environment Detection
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      environment: ${{ steps.env.outputs.environment }}
      main_env: ${{ steps.env.outputs.main_env }}
      should_deploy: ${{ steps.env.outputs.should_deploy }}
      is_numbered_env: ${{ steps.env.outputs.is_numbered_env }}
      ecr_registry: ${{ steps.env.outputs.ecr_registry }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Determine Environment and AWS Registry
        id: env
        run: |
          # Determine environment based on branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            ENVIRONMENT="prod"
            MAIN_ENV="prod"
            SHOULD_DEPLOY="false"  # Main branch requires manual deployment
            IS_NUMBERED_ENV="false"
          elif [[ "${{ github.ref_name }}" =~ ^dev[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="dev"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^qa[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="qa"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^staging[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="staging"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^hotfix[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="hotfix"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          else
            echo "âŒ Invalid branch name: ${{ github.ref_name }}"
            echo "âŒ Only these formats are allowed: dev##, qa##, staging##, hotfix##, main"
            exit 1
          fi

          # Get AWS Account ID dynamically
          AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)

          # Set ECR registry for later use
          ECR_REGISTRY="${AWS_ACCOUNT_ID}.dkr.ecr.${{ env.AWS_REGION }}.amazonaws.com"

          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "main_env=$MAIN_ENV" >> $GITHUB_OUTPUT
          echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
          echo "is_numbered_env=$IS_NUMBERED_ENV" >> $GITHUB_OUTPUT
          echo "ecr_registry=$ECR_REGISTRY" >> $GITHUB_OUTPUT

          echo "ðŸŽ¯ Environment: $ENVIRONMENT"
          echo "ðŸ—ï¸  Main Environment: $MAIN_ENV"
          echo "ðŸš€ Should Deploy: $SHOULD_DEPLOY"
          echo "ðŸ”— ECR Registry: $ECR_REGISTRY"

  # ===========================================
  # PARALLEL CI PHASE (All run simultaneously)
  # ===========================================

  security-scan:
    name: ðŸ›¡ï¸ Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.skip_tests != 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Cache security scan results
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/trivy
            ~/.cache/semgrep
          key: ${{ runner.os }}-security-${{ hashFiles('**/requirements.txt', '**/package*.json') }}
          restore-keys: |
            ${{ runner.os }}-security-

      - name: Run Trivy vulnerability scanner
        id: trivy
        uses: aquasecurity/trivy-action@0.28.0
        with:
          scan-type: "fs"
          scan-ref: "."
          format: "sarif"
          output: "trivy-results.sarif"
          severity: "CRITICAL,HIGH,MEDIUM"
          exit-code: "0"

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v3
        if: always()
        with:
          sarif_file: "trivy-results.sarif"

      - name: Display Trivy scan summary
        if: always()
        run: |
          if [ -f "trivy-results.sarif" ]; then
            echo "ðŸ“Š Trivy scan completed - check Security tab for detailed results"
            echo "ðŸ” Scan summary:"
            cat trivy-results.sarif | jq -r '.runs[0].results | length' | xargs -I {} echo "Found {} potential issues"
          else
            echo "âš ï¸ Trivy results file not found"
          fi

      - name: Run TruffleHog secret scanner
        id: trufflehog
        uses: trufflesecurity/trufflehog@v3.82.13
        with:
          path: ./
          base: ${{ github.event.repository.default_branch }}
          head: ${{ github.sha }}
          extra_args: --exclude-paths=.trufflehogignore --fail
        continue-on-error: false

      - name: Handle TruffleHog failure
        if: failure() && steps.trufflehog.outcome == 'failure'
        run: |
          echo "ðŸš¨ TruffleHog detected potential secrets!"
          echo "Review the scan results above and ensure no real secrets are committed."
          echo "If these are false positives, add them to .trufflehogignore"
          exit 1

      - name: Security scan summary
        if: always()
        run: |
          echo "ðŸ›¡ï¸ Security Scan Summary"
          echo "======================="

          if [ "${{ steps.trivy.outcome }}" == "success" ]; then
            echo "âœ… Trivy vulnerability scan: PASSED"
          else
            echo "âŒ Trivy vulnerability scan: FAILED"
          fi

          if [ "${{ steps.trufflehog.outcome }}" == "success" ]; then
            echo "âœ… TruffleHog secret scan: PASSED"
          else
            echo "âŒ TruffleHog secret scan: FAILED"
          fi

          echo ""
          echo "ðŸ“Š View detailed results in:"
          echo "   - Security tab: Repository â†’ Security â†’ Code scanning"
          echo "   - Job logs: Expand steps above for details"

  backend-tests:
    name: ðŸ Backend Tests (${{ matrix.python-version }})
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.skip_tests != 'true'
    defaults:
      run:
        working-directory: ./python-ml-service
    strategy:
      matrix:
        python-version: ["3.11"]
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Cache Python dependencies
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install --upgrade setuptools wheel
          pip install -r requirements.txt --use-pep517

      - name: Run tests
        run: |
          python -m pytest tests/ -v --cov=app --cov-report=xml

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        if: always()
        with:
          file: ./coverage.xml
          flags: backend
          name: backend-coverage
          fail_ci_if_error: false

  frontend-tests:
    name: âš›ï¸ Frontend Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15
    if: github.event.inputs.skip_tests != 'true'
    defaults:
      run:
        working-directory: ./apps/web
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: "npm"
          cache-dependency-path: apps/web/package-lock.json

      - name: Install dependencies
        run: npm ci

      - name: Check if test script exists
        id: check-tests
        run: |
          if npm run test --dry-run >/dev/null 2>&1; then
            echo "has_tests=true" >> $GITHUB_OUTPUT
          else
            echo "has_tests=false" >> $GITHUB_OUTPUT
            echo "âš ï¸ No test script found in package.json, skipping tests"
          fi

      - name: Run frontend tests
        if: steps.check-tests.outputs.has_tests == 'true'
        run: npm test -- --coverage --watchAll=false

      - name: Run linting and type checking (alternative to tests)
        if: steps.check-tests.outputs.has_tests == 'false'
        run: |
          echo "ðŸ” Running linting and type checking instead of tests..."
          npm run lint --if-present
          npm run type-check --if-present

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        if: always() && steps.check-tests.outputs.has_tests == 'true'
        with:
          directory: ./apps/web/coverage
          flags: frontend
          name: frontend-coverage
          fail_ci_if_error: false

  dependency-scan:
    name: ðŸ“¦ Dependency Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: github.event.inputs.skip_tests != 'true'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Run dependency vulnerability scan
        uses: pypa/gh-action-pip-audit@v1.0.8
        with:
          inputs: python-ml-service/requirements.txt

  terraform-plan:
    name: ðŸ—ï¸ Terraform Plan
    runs-on: ubuntu-latest
    timeout-minutes: 15
    needs: [detect-environment]
    if: needs.detect-environment.outputs.should_deploy == 'true'
    outputs:
      plan_id: ${{ steps.plan.outputs.plan_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Cache Terraform
        uses: actions/cache@v4
        with:
          path: |
            .terraform
            .terraform.lock.hcl
          key: ${{ runner.os }}-terraform-${{ hashFiles('**/terraform/**/*.tf') }}
          restore-keys: |
            ${{ runner.os }}-terraform-

      - name: Terraform Init
        working-directory: terraform/environments/${{ needs.detect-environment.outputs.main_env }}
        run: |
          terraform init \
            -backend-config="bucket=hibiji-terraform-state" \
            -backend-config="key=${{ needs.detect-environment.outputs.environment }}/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Terraform Plan
        id: plan
        working-directory: terraform/environments/${{ needs.detect-environment.outputs.main_env }}
        run: |
          terraform plan \
            -var="environment=${{ needs.detect-environment.outputs.environment }}" \
            -var="domain_name=${{ env.DOMAIN }}" \
            -var="aws_region=${{ env.AWS_REGION }}" \
            -out=tfplan

          # Generate readable plan
          terraform show -no-color tfplan > plan.txt
          echo "plan_id=tfplan" >> $GITHUB_OUTPUT

      - name: Upload Terraform Plan
        uses: actions/upload-artifact@v4
        with:
          name: terraform-plan-${{ needs.detect-environment.outputs.environment }}
          path: |
            terraform/environments/${{ needs.detect-environment.outputs.main_env }}/tfplan
            terraform/environments/${{ needs.detect-environment.outputs.main_env }}/plan.txt

  # ===========================================
  # BUILD & PACKAGE PHASE
  # ===========================================
  build-and-package:
    name: ðŸ—ï¸ Build & Package
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs:
      [
        detect-environment,
        security-scan,
        backend-tests,
        frontend-tests,
        dependency-scan,
        terraform-plan,
      ]
    if: |
      always() && 
      needs.detect-environment.result == 'success' &&
      (needs.security-scan.result == 'success' || github.event.inputs.skip_tests == 'true') &&
      (needs.backend-tests.result == 'success' || github.event.inputs.skip_tests == 'true') &&
      (needs.frontend-tests.result == 'success' || github.event.inputs.skip_tests == 'true') &&
      (needs.dependency-scan.result != 'cancelled' || github.event.inputs.skip_tests == 'true' || github.event.inputs.force_deploy == 'true') &&
      (needs.terraform-plan.result == 'success' || github.event.inputs.force_deploy == 'true')
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Clean up disk space
        run: |
          # Remove unnecessary files to free up space
          sudo rm -rf /usr/share/dotnet
          sudo rm -rf /opt/ghc
          sudo rm -rf "/usr/local/share/boost"
          sudo rm -rf "$AGENT_TOOLSDIRECTORY"
          df -h

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Create ECR repositories if they don't exist
        run: |
          # Create backend repository
          aws ecr describe-repositories --repository-names hibiji-backend --region ${{ env.AWS_REGION }} || \
          aws ecr create-repository \
            --repository-name hibiji-backend \
            --region ${{ env.AWS_REGION }} \
            --tags Key=Environment,Value=${{ needs.detect-environment.outputs.environment }} \
                   Key=Project,Value=hibiji \
                   Key=ManagedBy,Value=github-actions

          # Create frontend repository  
          aws ecr describe-repositories --repository-names hibiji-frontend --region ${{ env.AWS_REGION }} || \
          aws ecr create-repository \
            --repository-name hibiji-frontend \
            --region ${{ env.AWS_REGION }} \
            --tags Key=Environment,Value=${{ needs.detect-environment.outputs.environment }} \
                   Key=Project,Value=hibiji \
                   Key=ManagedBy,Value=github-actions

      - name: Build and push backend Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          platforms: linux/amd64
          push: true
          tags: |
            ${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-backend:${{ github.sha }}
            ${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-backend:latest
          # Use registry cache to avoid disk space issues
          cache-from: type=registry,ref=${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-backend:cache
          cache-to: type=registry,ref=${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-backend:cache,mode=max

      - name: Build and push frontend Docker image
        uses: docker/build-push-action@v5
        with:
          context: ./frontend
          platforms: linux/amd64
          push: true
          tags: |
            ${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-frontend:${{ github.sha }}
            ${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-frontend:latest
          # Use registry cache to avoid disk space issues
          cache-from: type=registry,ref=${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-frontend:cache
          cache-to: type=registry,ref=${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-frontend:cache,mode=max

      - name: Create build artifacts
        run: |
          mkdir -p build-artifacts
          echo "${{ github.sha }}" > build-artifacts/commit-sha.txt
          echo "${{ needs.detect-environment.outputs.environment }}" > build-artifacts/environment.txt
          echo "${{ needs.detect-environment.outputs.ecr_registry }}" > build-artifacts/ecr-registry.txt

      - name: Upload build artifacts
        uses: actions/upload-artifact@v4
        with:
          name: build-artifacts-${{ needs.detect-environment.outputs.environment }}
          path: build-artifacts/

  # ===========================================
  # DEPLOYMENT PHASE
  # ===========================================
  deploy:
    name: ðŸš€ Deploy to ${{ needs.detect-environment.outputs.environment }}
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [detect-environment, terraform-plan, build-and-package]
    if: |
      always() && 
      needs.detect-environment.outputs.should_deploy == 'true' &&
      needs.detect-environment.result == 'success' &&
      needs.terraform-plan.result == 'success' &&
      (needs.build-and-package.result == 'success' || github.event.inputs.force_deploy == 'true')
    environment:
      name: ${{ needs.detect-environment.outputs.environment }}
      url: https://${{ needs.detect-environment.outputs.environment }}.${{ env.DOMAIN }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ${{ env.TERRAFORM_VERSION }}

      - name: Download Terraform Plan
        uses: actions/download-artifact@v4
        with:
          name: terraform-plan-${{ needs.detect-environment.outputs.environment }}
          path: terraform/environments/${{ needs.detect-environment.outputs.main_env }}/

      - name: Create AWS Secrets if they don't exist
        run: |
          # Get the environment-specific prefix
          RESOURCE_PREFIX="hibiji-${{ needs.detect-environment.outputs.environment }}"

          # Check if secrets already exist (they may have random suffixes from Terraform)
          echo "ðŸ” Checking for existing secrets..."

          # Look for existing secret key (with pattern matching)
          EXISTING_SECRET_KEY=$(aws secretsmanager list-secrets \
            --region ${{ env.AWS_REGION }} \
            --query "SecretList[?starts_with(Name, '${RESOURCE_PREFIX}-secret-key')].Name" \
            --output text)

          # Look for existing database password (with pattern matching)
          EXISTING_DB_PASSWORD=$(aws secretsmanager list-secrets \
            --region ${{ env.AWS_REGION }} \
            --query "SecretList[?starts_with(Name, '${RESOURCE_PREFIX}-db-password')].Name" \
            --output text)

          # Create app secret key if it doesn't exist
          if [ -z "$EXISTING_SECRET_KEY" ]; then
            echo "ðŸ“ Creating new application secret key..."
            # Generate a random suffix to match Terraform naming convention
            RANDOM_SUFFIX=$(openssl rand -hex 4)
            SECRET_KEY_NAME="${RESOURCE_PREFIX}-secret-key-${RANDOM_SUFFIX}"
            
            aws secretsmanager create-secret \
              --name "$SECRET_KEY_NAME" \
              --description "Application secret key for ${{ needs.detect-environment.outputs.environment }}" \
              --secret-string "$(openssl rand -base64 64)" \
              --region ${{ env.AWS_REGION }} \
              --tags '[{"Key":"Environment","Value":"${{ needs.detect-environment.outputs.environment }}"},{"Key":"Project","Value":"hibiji"},{"Key":"ManagedBy","Value":"github-actions"}]'
            echo "âœ… Created secret: $SECRET_KEY_NAME"
          else
            echo "âœ… Found existing secret key: $EXISTING_SECRET_KEY"
          fi

          # Create database password if it doesn't exist
          if [ -z "$EXISTING_DB_PASSWORD" ]; then
            echo "ðŸ“ Creating new database password..."
            # Generate a random suffix to match Terraform naming convention
            RANDOM_SUFFIX=$(openssl rand -hex 4)
            DB_PASSWORD_NAME="${RESOURCE_PREFIX}-db-password-${RANDOM_SUFFIX}"
            
            aws secretsmanager create-secret \
              --name "$DB_PASSWORD_NAME" \
              --description "Database password for ${{ needs.detect-environment.outputs.environment }}" \
              --secret-string "$(openssl rand -base64 32)" \
              --region ${{ env.AWS_REGION }} \
              --tags '[{"Key":"Environment","Value":"${{ needs.detect-environment.outputs.environment }}"},{"Key":"Project","Value":"hibiji"},{"Key":"ManagedBy","Value":"github-actions"}]'
            echo "âœ… Created secret: $DB_PASSWORD_NAME"
          else
            echo "âœ… Found existing database password: $EXISTING_DB_PASSWORD"
          fi

          echo "ðŸŽ¯ Secret verification complete"

      - name: Create S3 Terraform Backend Bucket
        run: |
          # Create S3 bucket for Terraform state if it doesn't exist
          aws s3api head-bucket --bucket "hibiji-terraform-state" --region ${{ env.AWS_REGION }} || \
          aws s3api create-bucket \
            --bucket "hibiji-terraform-state" \
            --region ${{ env.AWS_REGION }} \
            --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}

          # Enable versioning
          aws s3api put-bucket-versioning \
            --bucket "hibiji-terraform-state" \
            --versioning-configuration Status=Enabled

          # Enable encryption
          aws s3api put-bucket-encryption \
            --bucket "hibiji-terraform-state" \
            --server-side-encryption-configuration '{
              "Rules": [
                {
                  "ApplyServerSideEncryptionByDefault": {
                    "SSEAlgorithm": "AES256"
                  }
                }
              ]
            }'

          # Apply tags
          aws s3api put-bucket-tagging \
            --bucket "hibiji-terraform-state" \
            --tagging '{"TagSet":[{"Key":"Environment","Value":"shared"},{"Key":"Project","Value":"hibiji"},{"Key":"ManagedBy","Value":"github-actions"}]}'

      - name: Terraform Init
        working-directory: terraform/environments/${{ needs.detect-environment.outputs.main_env }}
        run: |
          terraform init \
            -backend-config="bucket=hibiji-terraform-state" \
            -backend-config="key=${{ needs.detect-environment.outputs.environment }}/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"

      - name: Terraform Apply
        working-directory: terraform/environments/${{ needs.detect-environment.outputs.main_env }}
        run: |
          terraform apply -auto-approve tfplan

      - name: Get infrastructure outputs
        id: terraform-outputs
        working-directory: terraform/environments/${{ needs.detect-environment.outputs.main_env }}
        run: |
          set -e
          echo "ðŸ” Retrieving Terraform outputs..."

          # Function to validate and clean terraform output
          get_terraform_output() {
            local output_name="$1"
            local raw_output
            
            # Get raw output
            raw_output=$(terraform output -raw "$output_name" 2>/dev/null || echo "")
            
            # Check if output contains warning patterns (multiline or box characters)
            if [[ "$raw_output" == *"Warning: No outputs found"* ]] || \
               [[ "$raw_output" == *"â•·"* ]] || \
               [[ "$raw_output" == *"â”‚"* ]] || \
               [[ "$raw_output" == *"â•µ"* ]] || \
               [[ "$raw_output" =~ $'\n' ]]; then
              echo ""  # Return empty if it's a warning or multiline
            else
              # Clean any remaining formatting and return single line
              echo "$raw_output" | tr -d '\r\n' | sed 's/\x1b\[[0-9;]*m//g'
            fi
          }

          # Get outputs safely
          ALB_DNS=$(get_terraform_output "alb_dns_name")
          CLUSTER_NAME=$(get_terraform_output "cluster_name")

          # Validate and report outputs
          if [ -z "$ALB_DNS" ]; then
            echo "âš ï¸  Warning: alb_dns_name output not found or invalid"
            echo "ðŸ“‹ Available outputs:"
            terraform output 2>/dev/null | head -10 || echo "No outputs available"
            ALB_DNS=""
          else
            echo "âœ… Found ALB DNS: $ALB_DNS"
          fi

          if [ -z "$CLUSTER_NAME" ]; then
            echo "âš ï¸  Warning: ecs_cluster_name output not found or invalid"
            echo "ðŸ“‹ Available outputs:"
            terraform output 2>/dev/null | head -10 || echo "No outputs available"
            CLUSTER_NAME=""
          else
            echo "âœ… Found Cluster Name: $CLUSTER_NAME"
          fi

          # Set GitHub Actions outputs (guaranteed single-line, safe format)
          echo "alb_dns=${ALB_DNS}" >> $GITHUB_OUTPUT
          echo "cluster_name=${CLUSTER_NAME}" >> $GITHUB_OUTPUT

          echo "ðŸŽ¯ Terraform outputs retrieved successfully"

      - name: Download build artifacts
        uses: actions/download-artifact@v4
        with:
          name: build-artifacts-${{ needs.detect-environment.outputs.environment }}
          path: build-artifacts/

      - name: Deploy application to ECS
        run: |
          echo "ðŸš€ Starting ECS deployment..."

          # Validate required outputs exist
          CLUSTER_NAME="${{ steps.terraform-outputs.outputs.cluster_name }}"
          if [ -z "$CLUSTER_NAME" ]; then
            echo "âŒ Error: ECS cluster name not found in Terraform outputs"
            echo "Available outputs:"
            cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}
            terraform output 2>/dev/null || echo "No outputs available"
            exit 1
          fi

          echo "âœ… Using ECS cluster: $CLUSTER_NAME"

          # Get the latest backend task definition
          echo "ðŸ“‹ Retrieving current backend task definition..."
          BACKEND_TASK_DEF=$(aws ecs describe-task-definition \
            --task-definition "hibiji-${{ needs.detect-environment.outputs.environment }}-backend" \
            --region ${{ env.AWS_REGION }} \
            --query 'taskDefinition' \
            --output json | jq 'del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .placementConstraints, .compatibilities, .registeredAt, .registeredBy)')

          # Update backend image URI
          UPDATED_BACKEND_TASK_DEF=$(echo $BACKEND_TASK_DEF | jq \
            --arg BACKEND_IMAGE "${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-backend:${{ github.sha }}" \
            '.containerDefinitions |= map(
              if .name == "backend" then .image = $BACKEND_IMAGE
              else . end
            )')

          # Register new backend task definition
          echo $UPDATED_BACKEND_TASK_DEF > updated-backend-task-def.json
          NEW_BACKEND_TASK_DEF_ARN=$(aws ecs register-task-definition \
            --cli-input-json file://updated-backend-task-def.json \
            --region ${{ env.AWS_REGION }} \
            --query 'taskDefinition.taskDefinitionArn' \
            --output text)

          # Update backend service
          echo "ðŸ”„ Updating ECS backend service..."
          aws ecs update-service \
            --cluster "$CLUSTER_NAME" \
            --service "hibiji-${{ needs.detect-environment.outputs.environment }}-backend" \
            --task-definition "$NEW_BACKEND_TASK_DEF_ARN" \
            --region ${{ env.AWS_REGION }}

          # Handle frontend service (if it exists)
          echo "ðŸ“‹ Checking for frontend service..."
          if aws ecs describe-services \
            --cluster "$CLUSTER_NAME" \
            --services "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].serviceName' \
            --output text 2>/dev/null | grep -q "frontend"; then
            
            echo "ðŸ“‹ Retrieving current frontend task definition..."
            FRONTEND_TASK_DEF=$(aws ecs describe-task-definition \
              --task-definition "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
              --region ${{ env.AWS_REGION }} \
              --query 'taskDefinition' \
              --output json | jq 'del(.taskDefinitionArn, .revision, .status, .requiresAttributes, .placementConstraints, .compatibilities, .registeredAt, .registeredBy)')

            # Update frontend image URI
            UPDATED_FRONTEND_TASK_DEF=$(echo $FRONTEND_TASK_DEF | jq \
              --arg FRONTEND_IMAGE "${{ needs.detect-environment.outputs.ecr_registry }}/hibiji-frontend:${{ github.sha }}" \
              '.containerDefinitions |= map(
                if .name == "frontend" then .image = $FRONTEND_IMAGE
                else . end
              )')

            # Register new frontend task definition
            echo $UPDATED_FRONTEND_TASK_DEF > updated-frontend-task-def.json
            NEW_FRONTEND_TASK_DEF_ARN=$(aws ecs register-task-definition \
              --cli-input-json file://updated-frontend-task-def.json \
              --region ${{ env.AWS_REGION }} \
              --query 'taskDefinition.taskDefinitionArn' \
              --output text)

            # Update frontend service
            echo "ðŸ”„ Updating ECS frontend service..."
            aws ecs update-service \
              --cluster "$CLUSTER_NAME" \
              --service "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
              --task-definition "$NEW_FRONTEND_TASK_DEF_ARN" \
              --region ${{ env.AWS_REGION }}
          else
            echo "âš ï¸ Frontend service not found, skipping frontend update..."
          fi

          # Wait for backend service deployment to complete
          echo "â³ Waiting for backend service to stabilize..."
          echo "ðŸ” Checking backend service status before waiting..."
          aws ecs describe-services \
            --cluster "$CLUSTER_NAME" \
            --services "hibiji-${{ needs.detect-environment.outputs.environment }}-backend" \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].{serviceName:serviceName,status:status,runningCount:runningCount,pendingCount:pendingCount,desiredCount:desiredCount}' \
            --output table

          # Wait with extended timeout (default is 15 minutes, extending to 20 minutes)
          timeout 1200 aws ecs wait services-stable \
            --cluster "$CLUSTER_NAME" \
            --services "hibiji-${{ needs.detect-environment.outputs.environment }}-backend" \
            --region ${{ env.AWS_REGION }} \
            --cli-read-timeout 1200 \
            --cli-connect-timeout 60 || {
              echo "âš ï¸ Backend service stabilization timed out or failed, checking current status..."
              aws ecs describe-services \
                --cluster "$CLUSTER_NAME" \
                --services "hibiji-${{ needs.detect-environment.outputs.environment }}-backend" \
                --region ${{ env.AWS_REGION }} \
                --query 'services[0].{serviceName:serviceName,status:status,runningCount:runningCount,pendingCount:pendingCount,desiredCount:desiredCount,deployments:deployments[0].{status:status,rolloutState:rolloutState}}' \
                --output table
              
              # Check task status for debugging
              echo "ðŸ” Checking task status..."
              aws ecs list-tasks \
                --cluster "$CLUSTER_NAME" \
                --service-name "hibiji-${{ needs.detect-environment.outputs.environment }}-backend" \
                --region ${{ env.AWS_REGION }} \
                --query 'taskArns[0]' \
                --output text | xargs -I {} aws ecs describe-tasks \
                --cluster "$CLUSTER_NAME" \
                --tasks {} \
                --region ${{ env.AWS_REGION }} \
                --query 'tasks[0].{taskArn:taskArn,lastStatus:lastStatus,healthStatus:healthStatus,containers:containers[0].{name:name,lastStatus:lastStatus,healthStatus:healthStatus}}' \
                --output table || echo "No tasks found or task description failed"
              
              # Continue with deployment even if stabilization timed out
              echo "âš ï¸ Continuing with deployment despite stabilization timeout..."
            }

          # Wait for frontend service deployment to complete (if it exists)
          if aws ecs describe-services \
            --cluster "$CLUSTER_NAME" \
            --services "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
            --region ${{ env.AWS_REGION }} \
            --query 'services[0].serviceName' \
            --output text 2>/dev/null | grep -q "frontend"; then
            
            echo "â³ Waiting for frontend service to stabilize..."
            echo "ðŸ” Checking frontend service status before waiting..."
            aws ecs describe-services \
              --cluster "$CLUSTER_NAME" \
              --services "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
              --region ${{ env.AWS_REGION }} \
              --query 'services[0].{serviceName:serviceName,status:status,runningCount:runningCount,pendingCount:pendingCount,desiredCount:desiredCount}' \
              --output table
            
            # Wait with extended timeout
            timeout 1200 aws ecs wait services-stable \
              --cluster "$CLUSTER_NAME" \
              --services "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
              --region ${{ env.AWS_REGION }} \
              --cli-read-timeout 1200 \
              --cli-connect-timeout 60 || {
                echo "âš ï¸ Frontend service stabilization timed out or failed, checking current status..."
                aws ecs describe-services \
                  --cluster "$CLUSTER_NAME" \
                  --services "hibiji-${{ needs.detect-environment.outputs.environment }}-frontend" \
                  --region ${{ env.AWS_REGION }} \
                  --query 'services[0].{serviceName:serviceName,status:status,runningCount:runningCount,pendingCount:pendingCount,desiredCount:desiredCount,deployments:deployments[0].{status:status,rolloutState:rolloutState}}' \
                  --output table
                
                echo "âš ï¸ Continuing with deployment despite frontend stabilization timeout..."
              }
          else
            echo "âš ï¸ Frontend service not found, skipping frontend wait..."
          fi

          echo "âœ… ECS deployment completed successfully"

      - name: Run post-deployment health check
        run: |
          # Wait a bit for the service to be ready
          sleep 30

          # Health check
          HEALTH_URL="https://${{ needs.detect-environment.outputs.environment }}.${{ env.DOMAIN }}/health"

          for i in {1..10}; do
            if curl -f -s "$HEALTH_URL" > /dev/null; then
              echo "âœ… Health check passed"
              break
            else
              echo "â³ Health check attempt $i/10 failed, retrying in 30s..."
              sleep 30
            fi
          done

      - name: Deployment Summary
        if: always()
        run: |
          echo ""
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
          echo "ðŸš€ DEPLOYMENT SUMMARY"
          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

          if [ "${{ job.status }}" == "success" ]; then
            echo "âœ… Status: DEPLOYMENT SUCCESSFUL"
            echo "ðŸŽ‰ Deployment to ${{ needs.detect-environment.outputs.environment }} completed successfully!"
          else
            echo "âŒ Status: DEPLOYMENT FAILED"
            echo "âš ï¸  Some deployment steps failed - check logs above for details"
          fi

          echo ""
          echo "ðŸŒ APPLICATION URLS:"
          echo "   Primary: https://${{ needs.detect-environment.outputs.environment }}.${{ env.DOMAIN }}"

          # Get ALB DNS name for internal access
          ALB_DNS="${{ steps.terraform-outputs.outputs.alb_dns_name }}"
          if [ -n "$ALB_DNS" ] && [ "$ALB_DNS" != "" ]; then
            echo "   Internal: http://$ALB_DNS"
            echo "   (Use internal URL if custom domain DNS not configured)"
            
            echo ""
            echo "ðŸ”§ DNS CONFIGURATION REQUIRED:"
            echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
            echo "To enable custom domain access, add these DNS records to hibiji.com:"
            echo ""
            echo "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”"
            echo "â”‚ DNS Records for hibiji.com Domain Hosting Service          â”‚"
            echo "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤"
            echo "â”‚ Type: CNAME                                                 â”‚"
            echo "â”‚ Name: ${{ needs.detect-environment.outputs.environment }}  â”‚"
            echo "â”‚ Value: $ALB_DNS                                             â”‚"
            echo "â”‚ TTL: 300 (or your provider's default)                      â”‚"
            echo "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜"
            echo ""
            echo "ðŸ“ INSTRUCTIONS:"
            echo "1. Log into your hibiji.com DNS hosting provider"
            echo "2. Navigate to DNS management/records section"
            echo "3. Add a new CNAME record:"
            echo "   â€¢ Subdomain: ${{ needs.detect-environment.outputs.environment }}"
            echo "   â€¢ Points to: $ALB_DNS"
            echo "4. Save the record and wait for DNS propagation (5-30 minutes)"
            echo "5. Test: https://${{ needs.detect-environment.outputs.environment }}.${{ env.DOMAIN }}"
          else
            echo "   Internal: âš ï¸ Not available (ALB DNS output missing)"
            echo ""
            echo "âš ï¸  DNS CONFIGURATION:"
            echo "   Cannot provide DNS records - ALB DNS name not available"
            echo "   Check Terraform outputs for alb_dns_name"
          fi

          echo ""
          echo "ðŸ“Š INFRASTRUCTURE DETAILS:"
          echo "   Environment: ${{ needs.detect-environment.outputs.environment }}"
          echo "   ECS Cluster: ${{ steps.terraform-outputs.outputs.cluster_name }}"
          echo "   Database: RDS PostgreSQL"
          echo "   Security: AWS Secrets Manager"
          echo "   Containers: Backend + Frontend"
          echo "   Region: ${{ env.AWS_REGION }}"

          if [ "${{ job.status }}" == "success" ]; then
            echo ""
            echo "ðŸŽ¯ NEXT STEPS:"
            echo "1. Configure DNS records above (if not already done)"
            echo "2. Test application at both URLs"
            echo "3. Monitor logs for any issues"
            echo "4. All services are running and healthy!"
          else
            echo ""
            echo "ðŸ”§ TROUBLESHOOTING:"
            echo "1. Check job logs above for specific errors"
            echo "2. Verify AWS credentials and permissions"
            echo "3. Check Terraform state and resources"
            echo "4. Use internal URL for direct testing if available"
          fi

          echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"

  # ===========================================
  # ASYNCHRONOUS DEPENDENCY FIX MONITORING
  # ===========================================
  async-dependency-fix-checker:
    name: ðŸ” Async Dependency Fix Monitor
    runs-on: ubuntu-latest
    timeout-minutes: 10
    if: always() # Always run, regardless of other job results
    needs: [dependency-scan] # Only needs dependency-scan to exist, not to succeed
    continue-on-error: true # Don't fail workflow if this monitoring fails
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install monitoring tools
        run: |
          python -m pip install --upgrade pip
          pip install pip-audit packaging requests

      - name: Check for dependency vulnerability fixes
        id: fix_check
        run: |
          echo "ðŸ” Checking for dependency vulnerability fixes..."

          # Create results file
          mkdir -p monitoring/dependency-fixes
          RESULTS_FILE="monitoring/dependency-fixes/fix-check-$(date +%Y%m%d_%H%M%S).md"

          echo "# ðŸ” Dependency Fix Monitoring Report" > "$RESULTS_FILE"
          echo "**Generated**: $(date)" >> "$RESULTS_FILE"
          echo "**Workflow**: ${{ github.workflow }}" >> "$RESULTS_FILE"
          echo "**Run ID**: ${{ github.run_id }}" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"

          # Check urllib3 compatibility with current boto3
          echo "## ðŸ” urllib3 2.5.0+ Compatibility Check" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"

          # Test urllib3 upgrade in isolated environment
          python -c "
          import subprocess
          import sys
          import json

          print('Testing urllib3 2.5.0 compatibility...')

          # Get current boto3 version from requirements.txt
          with open('python-ml-service/requirements.txt', 'r') as f:
              content = f.read()
              for line in content.split('\n'):
                  if line.startswith('boto3=='):
                      boto3_version = line.split('==')[1]
                      print(f'Current boto3 version: {boto3_version}')
                      break

          # Test compatibility
          try:
              result = subprocess.run([
                  sys.executable, '-c', '''
          import subprocess
          import sys
          subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"urllib3>=2.5.0\", \"--dry-run\", \"--quiet\"])
          print(\"âœ… urllib3 2.5.0+ appears compatible!\")
          '''
              ], capture_output=True, text=True, timeout=30)
              
              if result.returncode == 0:
                  print('âœ… urllib3 2.5.0+ compatibility: COMPATIBLE')
                  with open('$RESULTS_FILE', 'a') as f:
                      f.write('**Status**: âœ… **COMPATIBLE** - urllib3 >= 2.5.0 can be safely upgraded!\\n\\n')
                      f.write('**Action Required**: Update requirements.txt to urllib3>=2.5.0\\n\\n')
              else:
                  print('âŒ urllib3 2.5.0+ compatibility: CONFLICT')
                  with open('$RESULTS_FILE', 'a') as f:
                      f.write('**Status**: âŒ **CONFLICT** - urllib3 >= 2.5.0 still has dependency conflicts\\n\\n')
                      f.write('**Reason**: ' + result.stderr.replace('\n', ' ') + '\\n\\n')
          except Exception as e:
              print(f'âš ï¸ urllib3 compatibility check failed: {e}')
              with open('$RESULTS_FILE', 'a') as f:
                  f.write(f'**Status**: âš ï¸ **ERROR** - Could not test compatibility: {str(e)}\\n\\n')
          " 2>&1 | tee -a "$RESULTS_FILE"

      - name: Check PyTorch security updates
        run: |
          RESULTS_FILE="monitoring/dependency-fixes/fix-check-$(date +%Y%m%d_%H%M%S).md"

          echo "## ðŸ” PyTorch Security Updates Check" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"

          # Check for PyTorch security updates
          python -c "
          import subprocess
          import json
          import sys

          print('ðŸ” Checking PyTorch security updates...')

          try:
              # Get latest PyTorch version
              result = subprocess.run([
                  sys.executable, '-c', 
                  'import subprocess; import json; result = subprocess.run([\"pip\", \"index\", \"versions\", \"torch\"], capture_output=True, text=True); print(result.stdout)'
              ], capture_output=True, text=True, timeout=20)
              
              if 'Available versions:' in result.stdout:
                  print('âœ… PyTorch version check successful')
                  with open('$RESULTS_FILE', 'a') as f:
                      f.write('**Latest PyTorch versions checked** âœ…\\n\\n')
              else:
                  print('ðŸ“‹ Using pip show for version info')
                  version_result = subprocess.run(['pip', 'show', 'torch'], capture_output=True, text=True)
                  with open('$RESULTS_FILE', 'a') as f:
                      f.write('**Current PyTorch**: Standard version check completed\\n\\n')
          except Exception as e:
              print(f'âš ï¸ PyTorch update check failed: {e}')
              with open('$RESULTS_FILE', 'a') as f:
                  f.write(f'**PyTorch Check**: âš ï¸ Could not check updates: {str(e)}\\n\\n')
          " 2>&1 | tee -a "$RESULTS_FILE"

      - name: Check for boto3/botocore updates
        run: |
          RESULTS_FILE="monitoring/dependency-fixes/fix-check-$(date +%Y%m%d_%H%M%S).md"

          echo "## ðŸ” boto3/botocore Updates Check" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"

          # Check if newer boto3 versions support urllib3 2.5.0+
          python -c "
          import subprocess
          import sys

          print('ðŸ” Checking boto3/botocore urllib3 compatibility...')

          try:
              # Test with latest boto3
              result = subprocess.run([
                  sys.executable, '-c', '''
          import subprocess
          import sys
          # Try installing latest boto3 with urllib3 2.5.0
          subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3\", \"urllib3>=2.5.0\", \"--dry-run\", \"--quiet\"])
          print(\"âœ… Latest boto3 + urllib3 2.5.0: COMPATIBLE\")
          '''
              ], capture_output=True, text=True, timeout=30)
              
              if result.returncode == 0:
                  print('âœ… boto3 + urllib3 2.5.0: COMPATIBLE')
                  with open('$RESULTS_FILE', 'a') as f:
                      f.write('**boto3 Compatibility**: âœ… **COMPATIBLE** - boto3 latest + urllib3 2.5.0+ works!\\n\\n')
                      f.write('**Recommendation**: Consider upgrading boto3 to latest version\\n\\n')
              else:
                  print('âŒ boto3 + urllib3 2.5.0: STILL INCOMPATIBLE')
                  with open('$RESULTS_FILE', 'a') as f:
                      f.write('**boto3 Compatibility**: âŒ **INCOMPATIBLE** - boto3 + urllib3 2.5.0+ still conflicts\\n\\n')
          except Exception as e:
              print(f'âš ï¸ boto3 compatibility check failed: {e}')
              with open('$RESULTS_FILE', 'a') as f:
                  f.write(f'**boto3 Check**: âš ï¸ Could not test: {str(e)}\\n\\n')
          " 2>&1 | tee -a "$RESULTS_FILE"

      - name: Generate fix recommendations
        run: |
          RESULTS_FILE="monitoring/dependency-fixes/fix-check-$(date +%Y%m%d_%H%M%S).md"

          echo "" >> "$RESULTS_FILE"
          echo "## ðŸŽ¯ Automated Recommendations" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"
          echo "1. **If urllib3 2.5.0+ shows COMPATIBLE**: Update requirements.txt and test" >> "$RESULTS_FILE"
          echo "2. **If boto3 shows COMPATIBLE**: Consider boto3 version upgrade" >> "$RESULTS_FILE"
          echo "3. **Monitor this report**: Check weekly for compatibility improvements" >> "$RESULTS_FILE"
          echo "4. **Manual verification**: Always test in development before production" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"
          echo "## ðŸ“‹ Security Context" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"
          echo "- **Current urllib3 vulnerability**: SSRF (moderate, internal use)" >> "$RESULTS_FILE"
          echo "- **Current torch vulnerability**: DoS (low, local attack, disputed)" >> "$RESULTS_FILE"
          echo "- **Deployment status**: âœ… Production deployed with monitoring" >> "$RESULTS_FILE"
          echo "- **Risk level**: ðŸŸ¡ LOW (comprehensive monitoring in place)" >> "$RESULTS_FILE"
          echo "" >> "$RESULTS_FILE"
          echo "---" >> "$RESULTS_FILE"
          echo "*This is an automated dependency fix monitoring report. For manual security scanning, run: \`./scripts/security-monitor.sh\`*" >> "$RESULTS_FILE"

          echo "ðŸ“‹ Dependency fix monitoring report generated: $RESULTS_FILE"

      - name: Upload dependency fix monitoring results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: dependency-fix-monitoring-${{ github.run_id }}
          path: monitoring/dependency-fixes/
          retention-days: 30

      - name: Post summary comment (if PR)
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const path = require('path');

            // Find the latest results file
            const monitoringDir = 'monitoring/dependency-fixes';
            if (fs.existsSync(monitoringDir)) {
              const files = fs.readdirSync(monitoringDir);
              const latestFile = files.find(f => f.startsWith('fix-check-'));
              
              if (latestFile) {
                const content = fs.readFileSync(path.join(monitoringDir, latestFile), 'utf8');
                
                await github.rest.issues.createComment({
                  issue_number: context.issue.number,
                  owner: context.repo.owner,
                  repo: context.repo.repo,
                  body: `## ðŸ” Asynchronous Dependency Fix Monitoring\n\n${content}\n\n*This monitoring runs independently and does not affect deployment success.*`
                });
              }
            }
