name: Deploy Serverless Architecture

on:
  push:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  pull_request:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  workflow_dispatch:
    inputs:
      force_deploy:
        description: "Force deployment even if tests fail"
        required: false
        default: false
        type: boolean
      skip_tests:
        description: "Skip test execution for emergency deployments"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  security-events: write
  actions: read
  contents: read

env:
  AWS_REGION: us-west-1
  NODE_VERSION: 20
  DOMAIN: hibiji.com

# Prevent multiple runs from conflicting
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ===========================================
  # ENVIRONMENT DETECTION
  # ===========================================
  detect-environment:
    name: 🔍 Environment Detection
    runs-on: ubuntu-latest
    timeout-minutes: 5
    outputs:
      environment: ${{ steps.env.outputs.environment }}
      main_env: ${{ steps.env.outputs.main_env }}
      should_deploy: ${{ steps.env.outputs.should_deploy }}
      is_numbered_env: ${{ steps.env.outputs.is_numbered_env }}
      api_url: ${{ steps.env.outputs.api_url }}
      s3_bucket: ${{ steps.env.outputs.s3_bucket }}
      function_name: ${{ steps.env.outputs.function_name }}
      log_group_name: ${{ steps.env.outputs.log_group_name }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Determine Environment and Resources
        id: env
        run: |
          # Determine environment based on branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            ENVIRONMENT="prod"
            MAIN_ENV="prod"
            SHOULD_DEPLOY="false"  # Main branch requires manual deployment
            IS_NUMBERED_ENV="false"
          elif [[ "${{ github.ref_name }}" =~ ^dev[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="dev"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^qa[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="qa"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^staging[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="staging"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^hotfix[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="hotfix"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          else
            echo "❌ Invalid branch name: ${{ github.ref_name }}"
            echo "Valid branches: main, dev01-dev99, qa01-qa99, staging01-staging99, hotfix01-hotfix99"
            exit 1
          fi

          # Generate dynamic resource names
          FUNCTION_NAME="$MAIN_ENV-$ENVIRONMENT-dpp-api"
          S3_BUCKET="$MAIN_ENV-$ENVIRONMENT-dpp-website"
          LOG_GROUP_NAME="/aws/lambda/$FUNCTION_NAME"

          # Generate API URL dynamically based on environment pattern
          if [[ "$ENVIRONMENT" == "prod" ]]; then
            API_URL="https://api.$DOMAIN"
          else
            API_URL="https://$ENVIRONMENT.$DOMAIN"
          fi

          echo "Environment: $ENVIRONMENT"
          echo "Main Environment: $MAIN_ENV"
          echo "Should Deploy: $SHOULD_DEPLOY"
          echo "Is Numbered Environment: $IS_NUMBERED_ENV"
          echo "Function Name: $FUNCTION_NAME"
          echo "S3 Bucket: $S3_BUCKET"
          echo "Log Group: $LOG_GROUP_NAME"
          echo "API URL: $API_URL"

          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "main_env=$MAIN_ENV" >> $GITHUB_OUTPUT
          echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
          echo "is_numbered_env=$IS_NUMBERED_ENV" >> $GITHUB_OUTPUT
          echo "api_url=$API_URL" >> $GITHUB_OUTPUT
          echo "s3_bucket=$S3_BUCKET" >> $GITHUB_OUTPUT
          echo "function_name=$FUNCTION_NAME" >> $GITHUB_OUTPUT
          echo "log_group_name=$LOG_GROUP_NAME" >> $GITHUB_OUTPUT

  # =================================
  # Build and Test
  # =================================
  build-and-test:
    name: Build and Test
    runs-on: ubuntu-latest
    needs: detect-environment
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Run linting
        run: npm run lint --if-present

      - name: Run type checking
        run: npm run type-check --if-present

      - name: Run tests
        run: npm run test --if-present

  # =================================
  # Build Frontend for Static Export
  # =================================
  build-frontend:
    name: Build Frontend
    runs-on: ubuntu-latest
    needs: [detect-environment, build-and-test]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    outputs:
      frontend-hash: ${{ steps.frontend-hash.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Build shared packages
        run: |
          npm run build --workspace=@digital-persona/shared
          npm run build --workspace=@digital-persona/database

      - name: Build frontend for static export
        env:
          NODE_ENV: production
          NEXT_BUILD_EXPORT: true
          # Use dynamic API URL from environment detection
          NEXT_PUBLIC_API_URL: ${{ needs.detect-environment.outputs.api_url }}
        run: |
          npm run build --workspace=apps/web

      - name: Generate frontend hash
        id: frontend-hash
        run: |
          cd apps/web/out
          HASH=$(find . -type f -name "*.html" -o -name "*.js" -o -name "*.css" | xargs sha256sum | sort | sha256sum | cut -d' ' -f1)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Upload frontend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: frontend-${{ needs.detect-environment.outputs.environment }}-${{ steps.frontend-hash.outputs.hash }}
          path: apps/web/out/
          retention-days: 7

  # =================================
  # Build Backend for Lambda
  # =================================
  build-backend:
    name: Build Backend
    runs-on: ubuntu-latest
    needs: [detect-environment, build-and-test]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    outputs:
      backend-hash: ${{ steps.backend-hash.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Build shared packages
        run: |
          npm run build --workspace=@digital-persona/shared
          npm run build --workspace=@digital-persona/database

      - name: Build backend
        run: npm run build --workspace=apps/server

      - name: Generate backend hash
        id: backend-hash
        run: |
          cd apps/server/dist
          HASH=$(find . -type f | xargs sha256sum | sort | sha256sum | cut -d' ' -f1)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Upload backend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: backend-${{ needs.detect-environment.outputs.environment }}-${{ steps.backend-hash.outputs.hash }}
          path: apps/server/dist/
          retention-days: 7

  # =================================
  # Deploy Infrastructure
  # =================================
  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [detect-environment, build-frontend, build-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Terraform Init
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          # Clean up any existing state issues
          echo "🧹 Cleaning up potential state issues..."
          rm -f .terraform.lock.hcl
          rm -rf .terraform/

          # Create S3 backend bucket if it doesn't exist
          echo "📦 Ensuring S3 backend bucket exists..."
          aws s3api head-bucket --bucket "hibiji-terraform-state" --region ${{ env.AWS_REGION }} || \
          aws s3api create-bucket \
            --bucket "hibiji-terraform-state" \
            --region ${{ env.AWS_REGION }} \
            --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}

          # Enable versioning on the bucket
          aws s3api put-bucket-versioning \
            --bucket "hibiji-terraform-state" \
            --versioning-configuration Status=Enabled

          # Check if corrupted state exists and backup/remove it
          echo "🔍 Checking for corrupted state file..."
          STATE_KEY="${{ needs.detect-environment.outputs.main_env }}/serverless/terraform.tfstate"

          if aws s3api head-object --bucket "hibiji-terraform-state" --key "$STATE_KEY" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "⚠️ Found existing state file, backing up and removing corrupted state..."
            aws s3api copy-object \
              --bucket "hibiji-terraform-state" \
              --copy-source "hibiji-terraform-state/$STATE_KEY" \
              --key "${STATE_KEY}.backup.$(date +%Y%m%d-%H%M%S)" \
              --region ${{ env.AWS_REGION }}
            
            # Remove the corrupted state file
            aws s3api delete-object \
              --bucket "hibiji-terraform-state" \
              --key "$STATE_KEY" \
              --region ${{ env.AWS_REGION }}
            
            echo "✅ Corrupted state file backed up and removed"
          else
            echo "✅ No existing state file found"
          fi

          # Initialize with explicit backend configuration
          echo " Initializing Terraform..."
          terraform init \
            -backend-config="bucket=hibiji-terraform-state" \
            -backend-config="key=$STATE_KEY" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -reconfigure

      - name: Terraform Plan
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}
          terraform plan -out=tfplan

      - name: Import Existing Resources (IAM Roles & CloudWatch Log Groups)
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "🔍 Checking and importing existing resources..."

          # Import ECS execution role if it exists in AWS but not in state
          ECS_EXECUTION_ROLE="${{ needs.detect-environment.outputs.main_env }}-${{ needs.detect-environment.outputs.environment }}-dpp-ecs-execution"
          if aws iam get-role --role-name "$ECS_EXECUTION_ROLE" >/dev/null 2>&1; then
            if ! terraform state show aws_iam_role.ecs_execution >/dev/null 2>&1; then
              echo "📥 Importing ECS execution role: $ECS_EXECUTION_ROLE"
              terraform import aws_iam_role.ecs_execution "$ECS_EXECUTION_ROLE" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import ECS task role if it exists in AWS but not in state  
          ECS_TASK_ROLE="${{ needs.detect-environment.outputs.main_env }}-${{ needs.detect-environment.outputs.environment }}-dpp-ecs-task"
          if aws iam get-role --role-name "$ECS_TASK_ROLE" >/dev/null 2>&1; then
            if ! terraform state show aws_iam_role.ecs_task >/dev/null 2>&1; then
              echo " Importing ECS task role: $ECS_TASK_ROLE"
              terraform import aws_iam_role.ecs_task "$ECS_TASK_ROLE" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import Lambda execution role if it exists in AWS but not in state
          LAMBDA_EXECUTION_ROLE="${{ needs.detect-environment.outputs.main_env }}-${{ needs.detect-environment.outputs.environment }}-dpp-lambda-execution"
          if aws iam get-role --role-name "$LAMBDA_EXECUTION_ROLE" >/dev/null 2>&1; then
            if ! terraform state show module.lambda_backend.aws_iam_role.lambda_execution >/dev/null 2>&1; then
              echo "📥 Importing Lambda execution role: $LAMBDA_EXECUTION_ROLE"
              terraform import module.lambda_backend.aws_iam_role.lambda_execution "$LAMBDA_EXECUTION_ROLE" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import CloudWatch Log Group if it exists in AWS but not in state
          LAMBDA_LOG_GROUP="${{ needs.detect-environment.outputs.log_group_name }}"
          if aws logs describe-log-groups --log-group-name-prefix "$LAMBDA_LOG_GROUP" --query 'logGroups[?logGroupName==`'"$LAMBDA_LOG_GROUP"'`]' --output text | grep -q "$LAMBDA_LOG_GROUP"; then
            if ! terraform state show module.lambda_backend.aws_cloudwatch_log_group.lambda_logs >/dev/null 2>&1; then
              echo "📥 Importing CloudWatch Log Group: $LAMBDA_LOG_GROUP"
              terraform import module.lambda_backend.aws_cloudwatch_log_group.lambda_logs "$LAMBDA_LOG_GROUP" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import API Gateway Log Group if it exists in AWS but not in state
          API_LOG_GROUP="/aws/apigateway/${{ needs.detect-environment.outputs.function_name }}"
          if aws logs describe-log-groups --log-group-name-prefix "$API_LOG_GROUP" --query 'logGroups[?logGroupName==`'"$API_LOG_GROUP"'`]' --output text | grep -q "$API_LOG_GROUP"; then
            if ! terraform state show module.api_gateway.aws_cloudwatch_log_group.api_gateway >/dev/null 2>&1; then
              echo "📥 Importing API Gateway Log Group: $API_LOG_GROUP"
              terraform import module.api_gateway.aws_cloudwatch_log_group.api_gateway "$API_LOG_GROUP" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          echo "✅ Resource import step completed"

      - name: Terraform Apply with Fallback Strategy
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "🚀 Attempting full Terraform apply..."

          # Try full terraform apply first
          if terraform apply -input=false tfplan; then
            echo "✅ Full Terraform apply successful"
          else
            echo "⚠️ Full Terraform apply failed - trying targeted deployment for serverless components..."
            
            # Define core serverless resources that should work without IAM conflicts
            CORE_SERVERLESS_TARGETS=(
              "module.api_gateway.aws_apigatewayv2_api.main"
              "module.api_gateway.aws_apigatewayv2_integration.lambda_api"
              "module.api_gateway.aws_apigatewayv2_route.api_trpc"
              "module.api_gateway.aws_apigatewayv2_route.health"
              "module.api_gateway.aws_apigatewayv2_stage.main"
              "module.lambda_backend.aws_lambda_function.api"
              "module.lambda_backend.aws_cloudwatch_log_group.lambda_logs"
              "module.s3_website.aws_s3_bucket.website"
              "module.s3_website.aws_s3_bucket.builds"
              "module.s3_website.aws_cloudfront_distribution.website"
              "aws_s3_bucket.uploads"
              "aws_route53_record.website"
              "aws_route53_record.api"
              "aws_secretsmanager_secret.jwt_secret"
              "aws_secretsmanager_secret.database_password"
              "aws_secretsmanager_secret_version.jwt_secret"
              "aws_secretsmanager_secret_version.database_password"
            )
            
            # Build target flags
            TARGET_FLAGS=""
            for target in "${CORE_SERVERLESS_TARGETS[@]}"; do
              TARGET_FLAGS="$TARGET_FLAGS -target=$target"
            done
            
            echo "🔄 Applying targeted serverless deployment..."
            if terraform apply $TARGET_FLAGS -auto-approve; then
              echo "✅ Targeted serverless deployment successful"
            else
              echo "❌ Both full and targeted deployments failed"
              exit 1
            fi
          fi

  # =================================
  # Deploy Frontend
  # =================================
  deploy-frontend:
    name: Deploy Frontend
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-infrastructure, build-frontend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download frontend artifacts
        uses: actions/download-artifact@v4
        with:
          name: frontend-${{ needs.detect-environment.outputs.environment }}-${{ needs.build-frontend.outputs.frontend-hash }}

      - name: Deploy to S3
        run: |
          # Use dynamic S3 bucket name from environment detection
          aws s3 sync out/ s3://${{ needs.detect-environment.outputs.s3_bucket }} --delete

      - name: Invalidate CloudFront cache
        run: |
          # Use dynamic domain name based on environment
          DOMAIN_NAME="${{ needs.detect-environment.outputs.environment }}.${{ env.DOMAIN }}"
          DISTRIBUTION_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?Aliases.Items[?contains(@, '$DOMAIN_NAME')]].Id" --output text)
          if [ "$DISTRIBUTION_ID" != "None" ] && [ "$DISTRIBUTION_ID" != "" ]; then
            echo " Invalidating CloudFront cache for distribution: $DISTRIBUTION_ID"
            aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID --paths "/*"
          else
            echo "⚠️ No CloudFront distribution found for domain: $DOMAIN_NAME"
          fi

  # =================================
  # Deploy Backend
  # =================================
  deploy-backend:
    name: Deploy Backend
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-infrastructure, build-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download backend artifacts
        uses: actions/download-artifact@v4
        with:
          name: backend-${{ needs.detect-environment.outputs.environment }}-${{ needs.build-backend.outputs.backend-hash }}

      - name: Create Lambda deployment package
        run: |
          # Create deployment directory
          mkdir -p lambda-deployment
          cd lambda-deployment

          # Copy built backend files
          cp -r ../dist/* .

          # Copy workspace package dist folders and package.json files
          echo "📦 Copying workspace packages..."
          cp -r ../../packages/shared/dist ./node_modules/@digital-persona/shared/
          cp ../../packages/shared/package.json ./node_modules/@digital-persona/shared/
          cp -r ../../packages/database/dist ./node_modules/@digital-persona/database/
          cp ../../packages/database/package.json ./node_modules/@digital-persona/database/

          # Copy production dependencies
          echo " Copying production dependencies..."
          cp -r ../../node_modules/* ./node_modules/

          # Clean up unnecessary files
          echo "🧹 Cleaning up deployment package..."
          find . -name "*.d.ts" -delete
          find . -name "*.d.mts" -delete
          find . -name "*.d.cts" -delete
          find . -name "*.map" -delete
          find . -name "*.test.js" -delete
          find . -name "*.spec.js" -delete
          find . -name "__tests__" -type d -exec rm -rf {} + 2>/dev/null || true

          # Create deployment zip
          zip -r ../lambda-deployment.zip . -x "*.git*" "*.DS_Store*"

          cd ..

      - name: Deploy Lambda function
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"

          echo "🔄 Updating Lambda function: $FUNCTION_NAME"

          # Wait for Lambda to be ready
          check_lambda_ready() {
            local status=$(aws lambda get-function --function-name $FUNCTION_NAME --query 'Configuration.State' --output text 2>/dev/null)
            echo "Lambda status: $status"
            [[ "$status" == "Active" ]]
          }

          # Wait for Lambda to be ready before updating
          echo "⏳ Waiting for Lambda to be ready..."
          for i in {1..30}; do
            if check_lambda_ready; then
              echo "✅ Lambda is ready for update"
              break
            fi
            echo "⏳ Waiting... (attempt $i/30)"
            sleep 10
          done

          # Update Lambda with retry logic
          update_lambda() {
            local max_attempts=5
            local attempt=1
            
            while [ $attempt -le $max_attempts ]; do
              echo "🔄 Attempting Lambda update (attempt $attempt/$max_attempts)..."
              
              if aws lambda update-function-code --function-name $FUNCTION_NAME --zip-file fileb://lambda-deployment.zip; then
                echo "✅ Lambda update successful"
                return 0
              else
                echo "❌ Lambda update failed (attempt $attempt/$max_attempts)"
                
                if [ $attempt -lt $max_attempts ]; then
                  echo "⏳ Waiting before retry..."
                  sleep $((attempt * 10))
                fi
                
                attempt=$((attempt + 1))
              fi
            done
            
            echo "❌ All Lambda update attempts failed"
            return 1
          }

          update_lambda

      - name: Wait for Lambda update
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
          aws lambda wait function-updated --function-name $FUNCTION_NAME

  # =================================
  # Health Check
  # =================================
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-frontend, deploy-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get API Gateway URL
        id: api-url
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
          API_ID=$(aws apigatewayv2 get-apis --query "Items[?Name=='$FUNCTION_NAME'].ApiId" --output text)
          echo "api_url=https://$API_ID.execute-api.${{ env.AWS_REGION }}.amazonaws.com/v1" >> $GITHUB_OUTPUT

      - name: Test API health endpoint
        run: |
          API_URL="${{ steps.api-url.outputs.api_url }}/health"
          echo "Testing API health endpoint: $API_URL"

          # Test with detailed error capture
          for attempt in {1..5}; do
            echo "🔄 Health check attempt $attempt/5..."
            
            # Capture full response
            RESPONSE=$(curl -s -w "\nHTTP_STATUS:%{http_code}\nTIME:%{time_total}" "$API_URL" 2>&1)
            HTTP_STATUS=$(echo "$RESPONSE" | grep "HTTP_STATUS:" | cut -d: -f2)
            RESPONSE_TIME=$(echo "$RESPONSE" | grep "TIME:" | cut -d: -f2)
            RESPONSE_BODY=$(echo "$RESPONSE" | sed '/HTTP_STATUS:/d' | sed '/TIME:/d')
            
            echo "Status: $HTTP_STATUS"
            echo "Response Time: $RESPONSE_TIME seconds"
            echo "Response Body: $RESPONSE_BODY"
            
            if [ "$HTTP_STATUS" = "200" ]; then
              echo "✅ Health check passed!"
              break
            else
              echo "❌ Health check failed (attempt $attempt/5)"
              
              if [ $attempt -lt 5 ]; then
                echo "⏳ Waiting 30 seconds before retry..."
                sleep 30
              else
                echo "❌ All health check attempts failed"
                
                # Capture CloudWatch logs for debugging
                echo "📋 Capturing CloudWatch logs for debugging..."
                FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
                LOG_GROUP_NAME="${{ needs.detect-environment.outputs.log_group_name }}"
                
                # Get the latest log stream
                LOG_STREAM=$(aws logs describe-log-streams \
                  --log-group-name "$LOG_GROUP_NAME" \
                  --order-by LastEventTime \
                  --descending \
                  --max-items 1 \
                  --query 'logStreams[0].logStreamName' \
                  --output text 2>/dev/null || echo "No log streams found")
                
                if [ "$LOG_STREAM" != "None" ] && [ "$LOG_STREAM" != "" ]; then
                  echo " Latest log stream: $LOG_STREAM"
                  aws logs get-log-events \
                    --log-group-name "$LOG_GROUP_NAME" \
                    --log-stream-name "$LOG_STREAM" \
                    --start-time $(date -d '10 minutes ago' +%s)000 \
                    --query 'events[*].message' \
                    --output text
                fi
                
                exit 1
              fi
            fi
          done
