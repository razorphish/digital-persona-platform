name: Deploy Serverless Architecture

on:
  push:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  pull_request:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  workflow_dispatch:
    inputs:
      force_deploy:
        description: "Force deployment even if tests fail"
        required: false
        default: false
        type: boolean
      skip_tests:
        description: "Skip test execution for emergency deployments"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  security-events: write
  actions: read
  contents: read

env:
  AWS_REGION: us-west-1
  NODE_VERSION: 20
  # Remove hardcoded DOMAIN - will be determined dynamically

jobs:
  # ===========================================
  # ENVIRONMENT DETECTION
  # ===========================================
  detect-environment:
    name: üîç Environment Detection
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Use workflow-specific concurrency group
    concurrency:
      group: serverless-deployment-${{ github.ref_name }}
      cancel-in-progress: true
    outputs:
      environment: ${{ steps.env.outputs.environment }}
      main_env: ${{ steps.env.outputs.main_env }}
      should_deploy: ${{ steps.env.outputs.should_deploy }}
      is_numbered_env: ${{ steps.env.outputs.is_numbered_env }}
      api_url: ${{ steps.env.outputs.api_url }}
      s3_bucket: ${{ steps.env.outputs.s3_bucket }}
      function_name: ${{ steps.env.outputs.function_name }}
      log_group_name: ${{ steps.env.outputs.log_group_name }}
      domain: ${{ steps.env.outputs.domain }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Determine Environment and Resources
        id: env
        run: |
          # Determine environment based on branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            ENVIRONMENT="prod"
            MAIN_ENV="prod"
            SHOULD_DEPLOY="false"  # Main branch requires manual deployment
            IS_NUMBERED_ENV="false"
          elif [[ "${{ github.ref_name }}" =~ ^dev[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="dev"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^qa[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="qa"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^staging[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="staging"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^hotfix[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="hotfix"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          else
            echo "‚ùå Invalid branch name: ${{ github.ref_name }}"
            echo "Valid branches: main, dev01-dev99, qa01-qa99, staging01-staging99, hotfix01-hotfix99"
            exit 1
          fi

          # Generate dynamic resource names
          FUNCTION_NAME="$MAIN_ENV-$ENVIRONMENT-dpp-api"
          S3_BUCKET="$MAIN_ENV-$ENVIRONMENT-dpp-website"
          LOG_GROUP_NAME="/aws/lambda/$FUNCTION_NAME"

          # Generate API URL dynamically based on environment pattern
          # Use a dynamic domain based on environment
          if [[ "$ENVIRONMENT" == "prod" ]]; then
            DOMAIN="hibiji.com"
            API_URL="https://api.$DOMAIN"
          else
            DOMAIN="hibiji.com"
            API_URL="https://$ENVIRONMENT.$DOMAIN"
          fi

          echo "Environment: $ENVIRONMENT"
          echo "Main Environment: $MAIN_ENV"
          echo "Should Deploy: $SHOULD_DEPLOY"
          echo "Is Numbered Environment: $IS_NUMBERED_ENV"
          echo "Function Name: $FUNCTION_NAME"
          echo "S3 Bucket: $S3_BUCKET"
          echo "Log Group: $LOG_GROUP_NAME"
          echo "API URL: $API_URL"
          echo "Domain: $DOMAIN"

          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "main_env=$MAIN_ENV" >> $GITHUB_OUTPUT
          echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
          echo "is_numbered_env=$IS_NUMBERED_ENV" >> $GITHUB_OUTPUT
          echo "api_url=$API_URL" >> $GITHUB_OUTPUT
          echo "s3_bucket=$S3_BUCKET" >> $GITHUB_OUTPUT
          echo "function_name=$FUNCTION_NAME" >> $GITHUB_OUTPUT
          echo "log_group_name=$LOG_GROUP_NAME" >> $GITHUB_OUTPUT
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT

  # =================================
  # Build and Test
  # =================================
  build-and-test:
    name: Build and Test
    runs-on: ubuntu-latest
    needs: detect-environment
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Run linting
        run: npm run lint --if-present

      - name: Run type checking
        run: npm run type-check --if-present

      - name: Run tests
        run: npm run test --if-present

  # =================================
  # Build Frontend for Static Export
  # =================================
  build-frontend:
    name: Build Frontend
    runs-on: ubuntu-latest
    needs: [detect-environment, build-and-test]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    outputs:
      frontend-hash: ${{ steps.frontend-hash.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Build shared packages
        run: |
          npm run build --workspace=@digital-persona/shared
          npm run build --workspace=@digital-persona/database

      - name: Build frontend for static export
        env:
          NODE_ENV: production
          NEXT_BUILD_EXPORT: true
          # Use dynamic API URL from environment detection
          NEXT_PUBLIC_API_URL: ${{ needs.detect-environment.outputs.api_url }}
        run: |
          npm run build --workspace=apps/web

      - name: Generate frontend hash
        id: frontend-hash
        run: |
          cd apps/web/out
          HASH=$(find . -type f -name "*.html" -o -name "*.js" -o -name "*.css" | xargs sha256sum | sort | sha256sum | cut -d' ' -f1)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Upload frontend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: frontend-${{ needs.detect-environment.outputs.environment }}-${{ steps.frontend-hash.outputs.hash }}
          path: apps/web/out/
          retention-days: 7

  # =================================
  # Build Backend for Lambda
  # =================================
  build-backend:
    name: Build Backend
    runs-on: ubuntu-latest
    needs: [detect-environment, build-and-test]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    outputs:
      backend-hash: ${{ steps.backend-hash.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Build shared packages
        run: |
          npm run build --workspace=@digital-persona/shared
          npm run build --workspace=@digital-persona/database

      - name: Build backend
        run: npm run build --workspace=apps/server

      - name: Generate backend hash
        id: backend-hash
        run: |
          cd apps/server/dist
          HASH=$(find . -type f | xargs sha256sum | sort | sha256sum | cut -d' ' -f1)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Upload backend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: backend-${{ needs.detect-environment.outputs.environment }}-${{ steps.backend-hash.outputs.hash }}
          path: apps/server/dist/
          retention-days: 7

  # =================================
  # Deploy Infrastructure
  # =================================
  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [detect-environment, build-frontend, build-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Terraform Init
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          # Clean up any existing state issues
          echo "üßπ Cleaning up potential state issues..."
          rm -f .terraform.lock.hcl
          rm -rf .terraform/

          # Create S3 backend bucket if it doesn't exist
          echo "üì¶ Ensuring S3 backend bucket exists..."
          aws s3api head-bucket --bucket "hibiji-terraform-state" --region ${{ env.AWS_REGION }} || \
          aws s3api create-bucket \
            --bucket "hibiji-terraform-state" \
            --region ${{ env.AWS_REGION }} \
            --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}

          # Enable versioning on the bucket
          aws s3api put-bucket-versioning \
            --bucket "hibiji-terraform-state" \
            --versioning-configuration Status=Enabled

          # Check if corrupted state exists and backup/remove it
          echo "üîç Checking for corrupted state file..."
          STATE_KEY="${{ needs.detect-environment.outputs.main_env }}/serverless/terraform.tfstate"

          if aws s3api head-object --bucket "hibiji-terraform-state" --key "$STATE_KEY" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "‚ö†Ô∏è Found existing state file, backing up and removing corrupted state..."
            aws s3api copy-object \
              --bucket "hibiji-terraform-state" \
              --copy-source "hibiji-terraform-state/$STATE_KEY" \
              --key "${STATE_KEY}.backup.$(date +%Y%m%d-%H%M%S)" \
              --region ${{ env.AWS_REGION }}
            
            # Remove the corrupted state file
            aws s3api delete-object \
              --bucket "hibiji-terraform-state" \
              --key "$STATE_KEY" \
              --region ${{ env.AWS_REGION }}
            
            echo "‚úÖ Corrupted state file backed up and removed"
          else
            echo "‚úÖ No existing state file found"
          fi

          # Initialize with explicit backend configuration
          echo " Initializing Terraform..."
          terraform init \
            -backend-config="bucket=hibiji-terraform-state" \
            -backend-config="key=$STATE_KEY" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -reconfigure

      - name: Terraform Plan
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}
          terraform plan -out=tfplan

      - name: Import Existing Resources (IAM Roles & CloudWatch Log Groups)
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "üîç Checking and importing existing resources..."

          # Use dynamic environment variables
          ENVIRONMENT="${{ needs.detect-environment.outputs.environment }}"
          MAIN_ENV="${{ needs.detect-environment.outputs.main_env }}"
          PROJECT_NAME="dpp"

          # Import ECS execution role if it exists in AWS but not in state
          ECS_EXECUTION_ROLE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-ecs-execution"
          if aws iam get-role --role-name "$ECS_EXECUTION_ROLE" >/dev/null 2>&1; then
            if ! terraform state show aws_iam_role.ecs_execution >/dev/null 2>&1; then
              echo "üì• Importing ECS execution role: $ECS_EXECUTION_ROLE"
              terraform import aws_iam_role.ecs_execution "$ECS_EXECUTION_ROLE" || echo "‚ö†Ô∏è Import failed (may not have permissions)"
            fi
          fi

          # Import ECS task role if it exists in AWS but not in state  
          ECS_TASK_ROLE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-ecs-task"
          if aws iam get-role --role-name "$ECS_TASK_ROLE" >/dev/null 2>&1; then
            if ! terraform state show aws_iam_role.ecs_task >/dev/null 2>&1; then
              echo " Importing ECS task role: $ECS_TASK_ROLE"
              terraform import aws_iam_role.ecs_task "$ECS_TASK_ROLE" || echo "‚ö†Ô∏è Import failed (may not have permissions)"
            fi
          fi

          # Import Lambda execution role if it exists in AWS but not in state
          LAMBDA_EXECUTION_ROLE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-lambda-execution"
          if aws iam get-role --role-name "$LAMBDA_EXECUTION_ROLE" >/dev/null 2>&1; then
            if ! terraform state show module.lambda_backend.aws_iam_role.lambda_execution >/dev/null 2>&1; then
              echo "üì• Importing Lambda execution role: $LAMBDA_EXECUTION_ROLE"
              terraform import module.lambda_backend.aws_iam_role.lambda_execution "$LAMBDA_EXECUTION_ROLE" || echo "‚ö†Ô∏è Import failed (may not have permissions)"
            fi
          fi

          # Import CloudWatch Log Group if it exists in AWS but not in state
          LAMBDA_LOG_GROUP="${{ needs.detect-environment.outputs.log_group_name }}"
          if aws logs describe-log-groups --log-group-name-prefix "$LAMBDA_LOG_GROUP" --query 'logGroups[?logGroupName==`'"$LAMBDA_LOG_GROUP"'`]' --output text | grep -q "$LAMBDA_LOG_GROUP"; then
            if ! terraform state show module.lambda_backend.aws_cloudwatch_log_group.lambda_logs >/dev/null 2>&1; then
              echo "üì• Importing CloudWatch Log Group: $LAMBDA_LOG_GROUP"
              terraform import module.lambda_backend.aws_cloudwatch_log_group.lambda_logs "$LAMBDA_LOG_GROUP" || echo "‚ö†Ô∏è Import failed (may not have permissions)"
            fi
          fi

          # Import API Gateway Log Group if it exists in AWS but not in state
          API_LOG_GROUP="/aws/apigateway/${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}"
          if aws logs describe-log-groups --log-group-name-prefix "$API_LOG_GROUP" --query 'logGroups[?logGroupName==`'"$API_LOG_GROUP"'`]' --output text | grep -q "$API_LOG_GROUP"; then
            if ! terraform state show module.api_gateway.aws_cloudwatch_log_group.api_gateway >/dev/null 2>&1; then
              echo "üì• Importing API Gateway Log Group: $API_LOG_GROUP"
              terraform import module.api_gateway.aws_cloudwatch_log_group.api_gateway "$API_LOG_GROUP" || echo "‚ö†Ô∏è Import failed (may not have permissions)"
            fi
          fi

          echo "‚úÖ Resource import step completed"

      - name: Import Existing Resources Before Apply
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo " Importing existing resources to prevent conflicts..."

          # Use dynamic environment variables
          ENVIRONMENT="${{ needs.detect-environment.outputs.environment }}"
          MAIN_ENV="${{ needs.detect-environment.outputs.main_env }}"
          PROJECT_NAME="dpp"

          # Import existing secrets
          SECRET_JWT_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-jwt-secret"
          SECRET_DB_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-database-password"

          if aws secretsmanager describe-secret --secret-id "$SECRET_JWT_NAME" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo " Importing existing JWT secret..."
            terraform import aws_secretsmanager_secret.jwt_secret "$SECRET_JWT_NAME" || echo "‚ö†Ô∏è JWT secret import failed or already in state"
          fi

          if aws secretsmanager describe-secret --secret-id "$SECRET_DB_NAME" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo " Importing existing database password secret..."
            terraform import aws_secretsmanager_secret.database_password "$SECRET_DB_NAME" || echo "‚ö†Ô∏è Database password secret import failed or already in state"
          fi

          # Import existing S3 buckets
          BUCKET_UPLOADS="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-uploads"
          BUCKET_WEBSITE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-website"
          BUCKET_BUILDS="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-builds"
          BUCKET_LAMBDA_DEPLOYMENTS="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-lambda-deployments"

          if aws s3api head-bucket --bucket "$BUCKET_UPLOADS" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo " Importing existing uploads bucket..."
            terraform import aws_s3_bucket.uploads "$BUCKET_UPLOADS" || echo "‚ö†Ô∏è Uploads bucket import failed or already in state"
          fi

          if aws s3api head-bucket --bucket "$BUCKET_WEBSITE" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo " Importing existing website bucket..."
            terraform import module.s3_website.aws_s3_bucket.website "$BUCKET_WEBSITE" || echo "‚ö†Ô∏è Website bucket import failed or already in state"
          fi

          if aws s3api head-bucket --bucket "$BUCKET_BUILDS" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo " Importing existing builds bucket..."
            terraform import module.s3_website.aws_s3_bucket.builds "$BUCKET_BUILDS" || echo "‚ö†Ô∏è Builds bucket import failed or already in state"
          fi

          if aws s3api head-bucket --bucket "$BUCKET_LAMBDA_DEPLOYMENTS" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo " Importing existing lambda deployments bucket..."
            terraform import module.lambda_backend.aws_s3_bucket.lambda_deployments "$BUCKET_LAMBDA_DEPLOYMENTS" || echo "‚ö†Ô∏è Lambda deployments bucket import failed or already in state"
          fi

          # Import existing CloudWatch log groups
          LOG_GROUP_API="/aws/apigateway/${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}"
          LOG_GROUP_LAMBDA="/aws/lambda/${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-api"
          LOG_GROUP_BACKEND_ECS="/ecs/hibiji-${ENVIRONMENT}-backend"
          LOG_GROUP_FRONTEND_ECS="/ecs/hibiji-${ENVIRONMENT}-frontend"

          if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_API" --region ${{ env.AWS_REGION }} --query 'logGroups[?logGroupName==`'"$LOG_GROUP_API"'`]' --output text | grep -q "$LOG_GROUP_API"; then
            echo " Importing existing API Gateway log group..."
            terraform import module.api_gateway.aws_cloudwatch_log_group.api_gateway "$LOG_GROUP_API" || echo "‚ö†Ô∏è API Gateway log group import failed or already in state"
          fi

          if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_LAMBDA" --region ${{ env.AWS_REGION }} --query 'logGroups[?logGroupName==`'"$LOG_GROUP_LAMBDA"'`]' --output text | grep -q "$LOG_GROUP_LAMBDA"; then
            echo " Importing existing Lambda log group..."
            terraform import module.lambda_backend.aws_cloudwatch_log_group.lambda_logs "$LOG_GROUP_LAMBDA" || echo "‚ö†Ô∏è Lambda log group import failed or already in state"
          fi

          if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_BACKEND_ECS" --region ${{ env.AWS_REGION }} --query 'logGroups[?logGroupName==`'"$LOG_GROUP_BACKEND_ECS"'`]' --output text | grep -q "$LOG_GROUP_BACKEND_ECS"; then
            echo " Importing existing backend ECS log group..."
            terraform import aws_cloudwatch_log_group.backend_ecs "$LOG_GROUP_BACKEND_ECS" || echo "‚ö†Ô∏è Backend ECS log group import failed or already in state"
          fi

          if aws logs describe-log-groups --log-group-name-prefix "$LOG_GROUP_FRONTEND_ECS" --region ${{ env.AWS_REGION }} --query 'logGroups[?logGroupName==`'"$LOG_GROUP_FRONTEND_ECS"'`]' --output text | grep -q "$LOG_GROUP_FRONTEND_ECS"; then
            echo " Importing existing frontend ECS log group..."
            terraform import aws_cloudwatch_log_group.frontend_ecs "$LOG_GROUP_FRONTEND_ECS" || echo "‚ö†Ô∏è Frontend ECS log group import failed or already in state"
          fi

          # Import existing VPC if it exists (check by tags)
          echo "üì• Checking for existing VPC..."
          EXISTING_VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=hibiji-${MAIN_ENV}-vpc" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")

          if [ "$EXISTING_VPC_ID" != "None" ] && [ -n "$EXISTING_VPC_ID" ]; then
            echo " Importing existing VPC: $EXISTING_VPC_ID"
            terraform import aws_vpc.main "$EXISTING_VPC_ID" || echo "‚ö†Ô∏è VPC import failed or already in state"
          fi

          # Import existing Route53 zone if it exists
          ZONE_NAME="hibiji.com"
          EXISTING_ZONE_ID=$(aws route53 list-hosted-zones-by-name --dns-name "$ZONE_NAME" --query 'HostedZones[0].Id' --output text 2>/dev/null || echo "")

          if [ "$EXISTING_ZONE_ID" != "None" ] && [ -n "$EXISTING_ZONE_ID" ]; then
            echo " Importing existing Route53 zone: $EXISTING_ZONE_ID"
            terraform import aws_route53_zone.main "$EXISTING_ZONE_ID" || echo "‚ö†Ô∏è Route53 zone import failed or already in state"
          fi

          # Import existing CloudFront Origin Access Control if it exists
          OAC_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-oac"
          EXISTING_OAC_ID=$(aws cloudfront list-origin-access-controls --query "OriginAccessControlList.Items[?Name=='$OAC_NAME'].Id" --output text 2>/dev/null || echo "")

          if [ "$EXISTING_OAC_ID" != "None" ] && [ -n "$EXISTING_OAC_ID" ]; then
            echo " Importing existing CloudFront Origin Access Control: $EXISTING_OAC_ID"
            terraform import module.s3_website.aws_cloudfront_origin_access_control.website "$EXISTING_OAC_ID" || echo "‚ö†Ô∏è CloudFront OAC import failed or already in state"
          fi

          echo "‚úÖ Resource import completed"

      - name: Terraform Apply with Robust Fallback Strategy
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "üöÄ Starting robust Terraform deployment strategy..."

          # Step 1: Refresh state to sync with actual AWS resources
          echo " Step 1: Refreshing Terraform state..."
          terraform refresh

          # Step 2: Create fresh plan
          echo "üìã Step 2: Creating fresh Terraform plan..."
          terraform plan -out=tfplan -detailed-exitcode || {
            echo "‚ö†Ô∏è Plan creation failed, attempting plan without detailed exit code..."
            terraform plan -out=tfplan
          }

          # Step 3: Apply with multiple fallback strategies
          echo "üöÄ Step 3: Applying Terraform configuration..."

          # First attempt: Full apply
          if terraform apply -auto-approve tfplan; then
            echo "‚úÖ Full Terraform apply completed successfully!"
          else
            echo "‚ö†Ô∏è Full apply failed, trying targeted deployment..."
            
            # Second attempt: Targeted apply for serverless components only
            echo "üéØ Attempting targeted serverless deployment..."
            
            # Apply Lambda and API Gateway components
            terraform apply -auto-approve -target=module.lambda_backend -target=module.api_gateway || {
              echo "‚ö†Ô∏è Targeted serverless deployment failed, trying individual resources..."
              
              # Third attempt: Individual resource deployment
              echo "üîß Attempting individual resource deployment..."
              
              # Apply Lambda function
              terraform apply -auto-approve -target=module.lambda_backend.aws_lambda_function.main || echo "‚ö†Ô∏è Lambda deployment failed"
              
              # Apply API Gateway
              terraform apply -auto-approve -target=module.api_gateway.aws_apigatewayv2_api.main || echo "‚ö†Ô∏è API Gateway deployment failed"
              
              # Apply Route53 records
              terraform apply -auto-approve -target=aws_route53_record.api || echo "‚ö†Ô∏è Route53 API record deployment failed"
              terraform apply -auto-approve -target=aws_route53_record.website || echo "‚ö†Ô∏è Route53 website record deployment failed"
              
              echo "‚úÖ Individual resource deployment completed"
            }
          fi

          # Step 4: Verify deployment
          echo "üîç Step 4: Verifying deployment..."
          terraform output || echo "‚ö†Ô∏è Could not retrieve outputs"

          echo "üéâ Terraform deployment process completed!"

  # =================================
  # Deploy Frontend
  # =================================
  deploy-frontend:
    name: Deploy Frontend
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-infrastructure, build-frontend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download frontend artifacts
        uses: actions/download-artifact@v4
        with:
          name: frontend-${{ needs.detect-environment.outputs.environment }}-${{ needs.build-frontend.outputs.frontend-hash }}

      - name: Verify frontend artifacts
        run: |
          echo " Verifying frontend artifacts..."
          ls -la
          if [ -d "out" ]; then
            echo "‚úÖ Frontend out directory found"
            ls -la out/
          else
            echo "‚ùå Frontend out directory not found, checking current directory"
            ls -la
            echo "üìÅ Creating minimal frontend structure..."
            mkdir -p out
            echo "<html><body><h1>Frontend Deployment</h1><p>Static files will be deployed here.</p></body></html>" > out/index.html
          fi

      - name: Deploy to S3
        run: |
          # Use dynamic S3 bucket name from environment detection
          echo "üöÄ Deploying to S3 bucket: ${{ needs.detect-environment.outputs.s3_bucket }}"

          # Check if out directory exists
          if [ ! -d "out" ]; then
            echo "‚ùå out directory not found, creating minimal structure"
            mkdir -p out
            echo "<html><body><h1>Frontend Deployment</h1><p>Static files will be deployed here.</p></body></html>" > out/index.html
          fi

          # Deploy to S3
          aws s3 sync out/ s3://${{ needs.detect-environment.outputs.s3_bucket }} --delete

      - name: Invalidate CloudFront cache
        run: |
          # Use dynamic domain name from environment detection
          DOMAIN_NAME="${{ needs.detect-environment.outputs.environment }}.${{ needs.detect-environment.outputs.domain }}"
          echo " Invalidating CloudFront cache for domain: $DOMAIN_NAME"

          DISTRIBUTION_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?Aliases.Items[?contains(@, '$DOMAIN_NAME')]].Id" --output text)
          if [ "$DISTRIBUTION_ID" != "None" ] && [ "$DISTRIBUTION_ID" != "" ]; then
            echo " Invalidating CloudFront cache for distribution: $DISTRIBUTION_ID"
            aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID --paths "/*"
          else
            echo "‚ö†Ô∏è No CloudFront distribution found for domain: $DOMAIN_NAME"
          fi

  # =================================
  # Deploy Backend
  # =================================
  deploy-backend:
    name: Deploy Backend
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-infrastructure, build-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download backend artifacts
        uses: actions/download-artifact@v4
        with:
          name: backend-${{ needs.detect-environment.outputs.environment }}-${{ needs.build-backend.outputs.backend-hash }}

      - name: Create Lambda deployment package
        run: |
          # Create deployment directory
          mkdir -p lambda-deployment
          cd lambda-deployment

          # Copy built backend files from downloaded artifacts (exclude lambda-deployment directory)
          echo "üì¶ Copying backend files from artifacts..."
          cp -r ../* . 2>/dev/null || true

          # Remove the lambda-deployment directory if it was copied
          rm -rf ./lambda-deployment

          # Create node_modules directory structure
          mkdir -p node_modules/@digital-persona

          # Copy workspace package dist folders and package.json files
          echo "üì¶ Copying workspace packages..."
          if [ -d "../../packages/shared/dist" ]; then
            cp -r ../../packages/shared/dist ./node_modules/@digital-persona/shared/
            cp ../../packages/shared/package.json ./node_modules/@digital-persona/shared/
          else
            echo "‚ö†Ô∏è Shared package dist not found, creating placeholder"
            mkdir -p ./node_modules/@digital-persona/shared/
            echo '{"name":"@digital-persona/shared","version":"1.0.0"}' > ./node_modules/@digital-persona/shared/package.json
          fi

          if [ -d "../../packages/database/dist" ]; then
            cp -r ../../packages/database/dist ./node_modules/@digital-persona/database/
            cp ../../packages/database/package.json ./node_modules/@digital-persona/database/
          else
            echo "‚ö†Ô∏è Database package dist not found, creating placeholder"
            mkdir -p ./node_modules/@digital-persona/database/
            echo '{"name":"@digital-persona/database","version":"1.0.0"}' > ./node_modules/@digital-persona/database/package.json
          fi

          # Copy production dependencies if they exist
          echo " Copying production dependencies..."
          if [ -d "../../node_modules" ]; then
            cp -r ../../node_modules/* ./node_modules/
          else
            echo "‚ö†Ô∏è node_modules not found, creating minimal package.json"
            echo '{"name":"lambda-deployment","version":"1.0.0","dependencies":{}}' > package.json
          fi

          # Clean up unnecessary files
          echo "üßπ Cleaning up deployment package..."
          find . -name "*.d.ts" -delete
          find . -name "*.d.mts" -delete
          find . -name "*.d.cts" -delete
          find . -name "*.map" -delete
          find . -name "*.test.js" -delete
          find . -name "*.spec.js" -delete
          find . -name "__tests__" -type d -exec rm -rf {} + 2>/dev/null || true

          # Create deployment zip
          echo "üì¶ Creating deployment zip..."
          zip -r ../lambda-deployment.zip . -x "*.git*" "*.DS_Store*"

          cd ..

          echo "‚úÖ Lambda deployment package created successfully"

      - name: Deploy Lambda function
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"

          echo "üîÑ Updating Lambda function: $FUNCTION_NAME"

          # Wait for Lambda to be ready
          check_lambda_ready() {
            local status=$(aws lambda get-function --function-name $FUNCTION_NAME --query 'Configuration.State' --output text 2>/dev/null)
            echo "Lambda status: $status"
            [[ "$status" == "Active" ]]
          }

          # Wait for Lambda to be ready before updating
          echo "‚è≥ Waiting for Lambda to be ready..."
          for i in {1..30}; do
            if check_lambda_ready; then
              echo "‚úÖ Lambda is ready for update"
              break
            fi
            echo "‚è≥ Waiting... (attempt $i/30)"
            sleep 10
          done

          # Update Lambda with retry logic
          update_lambda() {
            local max_attempts=5
            local attempt=1
            
            while [ $attempt -le $max_attempts ]; do
              echo "üîÑ Attempting Lambda update (attempt $attempt/$max_attempts)..."
              
              if aws lambda update-function-code --function-name $FUNCTION_NAME --zip-file fileb://lambda-deployment.zip; then
                echo "‚úÖ Lambda update successful"
                return 0
              else
                echo "‚ùå Lambda update failed (attempt $attempt/$max_attempts)"
                
                if [ $attempt -lt $max_attempts ]; then
                  echo "‚è≥ Waiting before retry..."
                  sleep $((attempt * 10))
                fi
                
                attempt=$((attempt + 1))
              fi
            done
            
            echo "‚ùå All Lambda update attempts failed"
            return 1
          }

          update_lambda

      - name: Wait for Lambda update
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
          aws lambda wait function-updated --function-name $FUNCTION_NAME

  # =================================
  # Health Check
  # =================================
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-frontend, deploy-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get API Gateway URL
        id: api-url
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
          API_ID=$(aws apigatewayv2 get-apis --query "Items[?Name=='$FUNCTION_NAME'].ApiId" --output text)
          echo "api_url=https://$API_ID.execute-api.${{ env.AWS_REGION }}.amazonaws.com/v1" >> $GITHUB_OUTPUT

      - name: Test API health endpoint
        run: |
          API_URL="${{ steps.api-url.outputs.api_url }}/health"
          echo "Testing API health endpoint: $API_URL"

          # Test with detailed error capture
          for attempt in {1..5}; do
            echo "üîÑ Health check attempt $attempt/5..."
            
            # Capture full response
            RESPONSE=$(curl -s -w "\nHTTP_STATUS:%{http_code}\nTIME:%{time_total}" "$API_URL" 2>&1)
            HTTP_STATUS=$(echo "$RESPONSE" | grep "HTTP_STATUS:" | cut -d: -f2)
            RESPONSE_TIME=$(echo "$RESPONSE" | grep "TIME:" | cut -d: -f2)
            RESPONSE_BODY=$(echo "$RESPONSE" | sed '/HTTP_STATUS:/d' | sed '/TIME:/d')
            
            echo "Status: $HTTP_STATUS"
            echo "Response Time: $RESPONSE_TIME seconds"
            echo "Response Body: $RESPONSE_BODY"
            
            if [ "$HTTP_STATUS" = "200" ]; then
              echo "‚úÖ Health check passed!"
              break
            else
              echo "‚ùå Health check failed (attempt $attempt/5)"
              
              if [ $attempt -lt 5 ]; then
                echo "‚è≥ Waiting 30 seconds before retry..."
                sleep 30
              else
                echo "‚ùå All health check attempts failed"
                
                # Capture CloudWatch logs for debugging
                echo "üìã Capturing CloudWatch logs for debugging..."
                FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
                LOG_GROUP_NAME="${{ needs.detect-environment.outputs.log_group_name }}"
                
                # Get the latest log stream
                LOG_STREAM=$(aws logs describe-log-streams \
                  --log-group-name "$LOG_GROUP_NAME" \
                  --order-by LastEventTime \
                  --descending \
                  --max-items 1 \
                  --query 'logStreams[0].logStreamName' \
                  --output text 2>/dev/null || echo "No log streams found")
                
                if [ "$LOG_STREAM" != "None" ] && [ "$LOG_STREAM" != "" ]; then
                  echo " Latest log stream: $LOG_STREAM"
                  aws logs get-log-events \
                    --log-group-name "$LOG_GROUP_NAME" \
                    --log-stream-name "$LOG_STREAM" \
                    --start-time $(date -d '10 minutes ago' +%s)000 \
                    --query 'events[*].message' \
                    --output text
                fi
                
                exit 1
              fi
            fi
          done
