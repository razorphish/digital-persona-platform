name: Deploy Serverless Architecture

on:
  push:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  pull_request:
    branches:
      - main
      - "dev[0-9][0-9]"
      - "qa[0-9][0-9]"
      - "staging[0-9][0-9]"
      - "hotfix[0-9][0-9]"
  workflow_dispatch:
    inputs:
      force_deploy:
        description: "Force deployment even if tests fail"
        required: false
        default: false
        type: boolean
      skip_tests:
        description: "Skip test execution for emergency deployments"
        required: false
        default: false
        type: boolean

permissions:
  id-token: write
  security-events: write
  actions: read
  contents: read

env:
  AWS_REGION: us-west-1
  NODE_VERSION: 20
  # Remove hardcoded DOMAIN - will be determined dynamically

jobs:
  # ===========================================
  # ENVIRONMENT DETECTION
  # ===========================================
  detect-environment:
    name: 🔍 Environment Detection
    runs-on: ubuntu-latest
    timeout-minutes: 5
    # Use workflow-specific concurrency group
    concurrency:
      group: serverless-deployment-${{ github.ref_name }}
      cancel-in-progress: true
    outputs:
      environment: ${{ steps.env.outputs.environment }}
      main_env: ${{ steps.env.outputs.main_env }}
      should_deploy: ${{ steps.env.outputs.should_deploy }}
      is_numbered_env: ${{ steps.env.outputs.is_numbered_env }}
      api_url: ${{ steps.env.outputs.api_url }}
      s3_bucket: ${{ steps.env.outputs.s3_bucket }}
      function_name: ${{ steps.env.outputs.function_name }}
      log_group_name: ${{ steps.env.outputs.log_group_name }}
      domain: ${{ steps.env.outputs.domain }}
    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Determine Environment and Resources
        id: env
        run: |
          # Determine environment based on branch
          if [[ "${{ github.ref }}" == "refs/heads/main" ]]; then
            ENVIRONMENT="prod"
            MAIN_ENV="prod"
            SHOULD_DEPLOY="false"  # Main branch requires manual deployment
            IS_NUMBERED_ENV="false"
          elif [[ "${{ github.ref_name }}" =~ ^dev[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="dev"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^qa[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="qa"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^staging[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="staging"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          elif [[ "${{ github.ref_name }}" =~ ^hotfix[0-9][0-9]$ ]]; then
            ENVIRONMENT="${{ github.ref_name }}"
            MAIN_ENV="hotfix"
            SHOULD_DEPLOY="true"
            IS_NUMBERED_ENV="true"
          else
            echo "❌ Invalid branch name: ${{ github.ref_name }}"
            echo "Valid branches: main, dev01-dev99, qa01-qa99, staging01-staging99, hotfix01-hotfix99"
            exit 1
          fi

          # Generate dynamic resource names
          FUNCTION_NAME="$MAIN_ENV-$ENVIRONMENT-dpp-api"
          S3_BUCKET="$MAIN_ENV-$ENVIRONMENT-dpp-website"
          LOG_GROUP_NAME="/aws/lambda/$FUNCTION_NAME"

          # Generate API URL dynamically based on environment pattern
          # Use a dynamic domain based on environment
          if [[ "$ENVIRONMENT" == "prod" ]]; then
            DOMAIN="hibiji.com"
            API_URL="https://api.$DOMAIN"
          else
            DOMAIN="hibiji.com"
            API_URL="https://$ENVIRONMENT.$DOMAIN"
          fi

          echo "Environment: $ENVIRONMENT"
          echo "Main Environment: $MAIN_ENV"
          echo "Should Deploy: $SHOULD_DEPLOY"
          echo "Is Numbered Environment: $IS_NUMBERED_ENV"
          echo "Function Name: $FUNCTION_NAME"
          echo "S3 Bucket: $S3_BUCKET"
          echo "Log Group: $LOG_GROUP_NAME"
          echo "API URL: $API_URL"
          echo "Domain: $DOMAIN"

          echo "environment=$ENVIRONMENT" >> $GITHUB_OUTPUT
          echo "main_env=$MAIN_ENV" >> $GITHUB_OUTPUT
          echo "should_deploy=$SHOULD_DEPLOY" >> $GITHUB_OUTPUT
          echo "is_numbered_env=$IS_NUMBERED_ENV" >> $GITHUB_OUTPUT
          echo "api_url=$API_URL" >> $GITHUB_OUTPUT
          echo "s3_bucket=$S3_BUCKET" >> $GITHUB_OUTPUT
          echo "function_name=$FUNCTION_NAME" >> $GITHUB_OUTPUT
          echo "log_group_name=$LOG_GROUP_NAME" >> $GITHUB_OUTPUT
          echo "domain=$DOMAIN" >> $GITHUB_OUTPUT

  # =================================
  # Build and Test
  # =================================
  build-and-test:
    name: Build and Test
    runs-on: ubuntu-latest
    needs: detect-environment
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Run linting
        run: npm run lint --if-present

      - name: Run type checking
        run: npm run type-check --if-present

      - name: Run tests
        run: npm run test --if-present

  # =================================
  # Build Frontend for Static Export
  # =================================
  build-frontend:
    name: Build Frontend
    runs-on: ubuntu-latest
    needs: [detect-environment, build-and-test]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    outputs:
      frontend-hash: ${{ steps.frontend-hash.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Build shared packages
        run: |
          npm run build --workspace=@digital-persona/shared
          npm run build --workspace=@digital-persona/database

      - name: Build frontend for static export
        env:
          NODE_ENV: production
          NEXT_BUILD_EXPORT: true
          # Use dynamic API URL from environment detection
          NEXT_PUBLIC_API_URL: ${{ needs.detect-environment.outputs.api_url }}
        run: |
          npm run build --workspace=apps/web

      - name: Generate frontend hash
        id: frontend-hash
        run: |
          cd apps/web/out
          HASH=$(find . -type f -name "*.html" -o -name "*.js" -o -name "*.css" | xargs sha256sum | sort | sha256sum | cut -d' ' -f1)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Upload frontend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: frontend-${{ needs.detect-environment.outputs.environment }}-${{ steps.frontend-hash.outputs.hash }}
          path: apps/web/out/
          retention-days: 7

  # =================================
  # Build Backend for Lambda
  # =================================
  build-backend:
    name: Build Backend
    runs-on: ubuntu-latest
    needs: [detect-environment, build-and-test]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    outputs:
      backend-hash: ${{ steps.backend-hash.outputs.hash }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          cache: npm

      - name: Install dependencies
        run: npm ci

      - name: Build shared packages
        run: |
          npm run build --workspace=@digital-persona/shared
          npm run build --workspace=@digital-persona/database

      - name: Build backend
        run: npm run build --workspace=apps/server

      - name: Generate backend hash
        id: backend-hash
        run: |
          cd apps/server/dist
          HASH=$(find . -type f | xargs sha256sum | sort | sha256sum | cut -d' ' -f1)
          echo "hash=$HASH" >> $GITHUB_OUTPUT

      - name: Upload backend artifacts
        uses: actions/upload-artifact@v4
        with:
          name: backend-${{ needs.detect-environment.outputs.environment }}-${{ steps.backend-hash.outputs.hash }}
          path: apps/server/dist/
          retention-days: 7

  # =================================
  # Deploy Infrastructure
  # =================================
  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [detect-environment, build-frontend, build-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.5.0"

      - name: Terraform Init
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          # Clean up any existing state issues
          echo "🧹 Cleaning up potential state issues..."
          rm -f .terraform.lock.hcl
          rm -rf .terraform/

          # Create S3 backend bucket if it doesn't exist
          echo "📦 Ensuring S3 backend bucket exists..."
          aws s3api head-bucket --bucket "hibiji-terraform-state" --region ${{ env.AWS_REGION }} || \
          aws s3api create-bucket \
            --bucket "hibiji-terraform-state" \
            --region ${{ env.AWS_REGION }} \
            --create-bucket-configuration LocationConstraint=${{ env.AWS_REGION }}

          # Enable versioning on the bucket
          aws s3api put-bucket-versioning \
            --bucket "hibiji-terraform-state" \
            --versioning-configuration Status=Enabled

          # Check if corrupted state exists and backup/remove it
          echo "🔍 Checking for corrupted state file..."
          STATE_KEY="${{ needs.detect-environment.outputs.main_env }}/serverless/terraform.tfstate"

          if aws s3api head-object --bucket "hibiji-terraform-state" --key "$STATE_KEY" --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "⚠️ Found existing state file, backing up and removing corrupted state..."
            aws s3api copy-object \
              --bucket "hibiji-terraform-state" \
              --copy-source "hibiji-terraform-state/$STATE_KEY" \
              --key "${STATE_KEY}.backup.$(date +%Y%m%d-%H%M%S)" \
              --region ${{ env.AWS_REGION }}
            
            # Remove the corrupted state file
            aws s3api delete-object \
              --bucket "hibiji-terraform-state" \
              --key "$STATE_KEY" \
              --region ${{ env.AWS_REGION }}
            
            echo "✅ Corrupted state file backed up and removed"
          else
            echo "✅ No existing state file found"
          fi

          # Initialize with explicit backend configuration
          echo " Initializing Terraform..."
          terraform init \
            -backend-config="bucket=hibiji-terraform-state" \
            -backend-config="key=$STATE_KEY" \
            -backend-config="region=${{ env.AWS_REGION }}" \
            -reconfigure

      - name: Terraform Plan
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}
          terraform plan -out=tfplan

      - name: Import Existing Resources (IAM Roles & CloudWatch Log Groups)
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "🔍 Checking and importing existing resources..."

          # Use dynamic environment variables
          ENVIRONMENT="${{ needs.detect-environment.outputs.environment }}"
          MAIN_ENV="${{ needs.detect-environment.outputs.main_env }}"
          PROJECT_NAME="dpp"

          # Import ECS execution role if it exists in AWS but not in state
          ECS_EXECUTION_ROLE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-ecs-execution"
          if aws iam get-role --role-name "$ECS_EXECUTION_ROLE" >/dev/null 2>&1; then
            if ! terraform state show aws_iam_role.ecs_execution >/dev/null 2>&1; then
              echo "📥 Importing ECS execution role: $ECS_EXECUTION_ROLE"
              terraform import aws_iam_role.ecs_execution "$ECS_EXECUTION_ROLE" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import ECS task role if it exists in AWS but not in state  
          ECS_TASK_ROLE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-ecs-task"
          if aws iam get-role --role-name "$ECS_TASK_ROLE" >/dev/null 2>&1; then
            if ! terraform state show aws_iam_role.ecs_task >/dev/null 2>&1; then
              echo " Importing ECS task role: $ECS_TASK_ROLE"
              terraform import aws_iam_role.ecs_task "$ECS_TASK_ROLE" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import Lambda execution role if it exists in AWS but not in state
          LAMBDA_EXECUTION_ROLE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-lambda-execution"
          if aws iam get-role --role-name "$LAMBDA_EXECUTION_ROLE" >/dev/null 2>&1; then
            if ! terraform state show module.lambda_backend.aws_iam_role.lambda_execution >/dev/null 2>&1; then
              echo "📥 Importing Lambda execution role: $LAMBDA_EXECUTION_ROLE"
              terraform import module.lambda_backend.aws_iam_role.lambda_execution "$LAMBDA_EXECUTION_ROLE" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import CloudWatch Log Group if it exists in AWS but not in state
          LAMBDA_LOG_GROUP="${{ needs.detect-environment.outputs.log_group_name }}"
          if aws logs describe-log-groups --log-group-name-prefix "$LAMBDA_LOG_GROUP" --query 'logGroups[?logGroupName==`'"$LAMBDA_LOG_GROUP"'`]' --output text | grep -q "$LAMBDA_LOG_GROUP"; then
            if ! terraform state show module.lambda_backend.aws_cloudwatch_log_group.lambda_logs >/dev/null 2>&1; then
              echo "📥 Importing CloudWatch Log Group: $LAMBDA_LOG_GROUP"
              terraform import module.lambda_backend.aws_cloudwatch_log_group.lambda_logs "$LAMBDA_LOG_GROUP" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          # Import API Gateway Log Group if it exists in AWS but not in state
          API_LOG_GROUP="/aws/apigateway/${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}"
          if aws logs describe-log-groups --log-group-name-prefix "$API_LOG_GROUP" --query 'logGroups[?logGroupName==`'"$API_LOG_GROUP"'`]' --output text | grep -q "$API_LOG_GROUP"; then
            if ! terraform state show module.api_gateway.aws_cloudwatch_log_group.api_gateway >/dev/null 2>&1; then
              echo "📥 Importing API Gateway Log Group: $API_LOG_GROUP"
              terraform import module.api_gateway.aws_cloudwatch_log_group.api_gateway "$API_LOG_GROUP" || echo "⚠️ Import failed (may not have permissions)"
            fi
          fi

          echo "✅ Resource import step completed"

      - name: Import Existing Resources Before Apply
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "🔄 Importing existing resources to prevent conflicts..."

          # Use dynamic environment variables
          ENVIRONMENT="${{ needs.detect-environment.outputs.environment }}"
          MAIN_ENV="${{ needs.detect-environment.outputs.main_env }}"
          PROJECT_NAME="dpp"

          # Function to check if resource is already in state
          check_resource_in_state() {
            local resource_address="$1"
            terraform state show "$resource_address" >/dev/null 2>&1
            return $?
          }

          # Function to check if AWS resource exists
          check_aws_resource() {
            local resource_type="$1"
            local resource_identifier="$2"
            
            case "$resource_type" in
              "secret")
                aws secretsmanager describe-secret --secret-id "$resource_identifier" --region ${{ env.AWS_REGION }} >/dev/null 2>&1
                ;;
              "s3_bucket")
                aws s3api head-bucket --bucket "$resource_identifier" --region ${{ env.AWS_REGION }} >/dev/null 2>&1
                ;;
              "log_group")
                aws logs describe-log-groups --log-group-name-prefix "$resource_identifier" --region ${{ env.AWS_REGION }} --query 'logGroups[?logGroupName==`'"$resource_identifier"'`]' --output text | grep -q "$resource_identifier"
                ;;
              "vpc")
                aws ec2 describe-vpcs --filters "Name=tag:Name,Values=$resource_identifier" --query 'Vpcs[0].VpcId' --output text --region ${{ env.AWS_REGION }} 2>/dev/null | grep -v "None"
                ;;
              "oac")
                aws cloudfront list-origin-access-controls --query "OriginAccessControlList.Items[?Name=='$resource_identifier'].Id" --output text 2>/dev/null | grep -v "None"
                ;;
            esac
          }

          # Function to safely import resource
          safe_import() {
            local resource_address="$1"
            local aws_resource_id="$2"
            local resource_type="$3"
            local description="$4"
            
            echo "🔍 Checking $description..."
            
            if check_resource_in_state "$resource_address"; then
              echo "✅ $description already in Terraform state"
              return 0
            fi
            
            if check_aws_resource "$resource_type" "$aws_resource_id"; then
              echo "📥 Importing $description: $aws_resource_id"
              if terraform import "$resource_address" "$aws_resource_id"; then
                echo "✅ Successfully imported $description"
              else
                echo "⚠️ Failed to import $description (may not have permissions)"
                return 1
              fi
            else
              echo "ℹ️ $description does not exist in AWS, will be created"
            fi
          }

          # Import existing secrets
          SECRET_JWT_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-jwt-secret"
          SECRET_DB_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-database-password"

          safe_import "aws_secretsmanager_secret.jwt_secret" "$SECRET_JWT_NAME" "secret" "JWT secret"
          safe_import "aws_secretsmanager_secret.database_password" "$SECRET_DB_NAME" "secret" "Database password secret"

          # Import existing S3 buckets
          BUCKET_UPLOADS="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-uploads"
          BUCKET_WEBSITE="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-website"
          BUCKET_BUILDS="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-builds"
          BUCKET_LAMBDA_DEPLOYMENTS="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-lambda-deployments"

          safe_import "aws_s3_bucket.uploads" "$BUCKET_UPLOADS" "s3_bucket" "Uploads bucket"
          safe_import "module.s3_website.aws_s3_bucket.website" "$BUCKET_WEBSITE" "s3_bucket" "Website bucket"
          safe_import "module.s3_website.aws_s3_bucket.builds" "$BUCKET_BUILDS" "s3_bucket" "Builds bucket"
          safe_import "module.lambda_backend.aws_s3_bucket.lambda_deployments" "$BUCKET_LAMBDA_DEPLOYMENTS" "s3_bucket" "Lambda deployments bucket"

          # Import existing CloudWatch log groups
          LOG_GROUP_API="/aws/apigateway/${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}"
          LOG_GROUP_LAMBDA="/aws/lambda/${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-api"
          LOG_GROUP_BACKEND_ECS="/ecs/hibiji-${ENVIRONMENT}-backend"
          LOG_GROUP_FRONTEND_ECS="/ecs/hibiji-${ENVIRONMENT}-frontend"

          safe_import "module.api_gateway.aws_cloudwatch_log_group.api_gateway" "$LOG_GROUP_API" "log_group" "API Gateway log group"
          safe_import "module.lambda_backend.aws_cloudwatch_log_group.lambda_logs" "$LOG_GROUP_LAMBDA" "log_group" "Lambda log group"
          safe_import "aws_cloudwatch_log_group.backend_ecs" "$LOG_GROUP_BACKEND_ECS" "log_group" "Backend ECS log group"
          safe_import "aws_cloudwatch_log_group.frontend_ecs" "$LOG_GROUP_FRONTEND_ECS" "log_group" "Frontend ECS log group"

          # Import existing VPC if it exists
          echo "📥 Checking for existing VPC..."
          VPC_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-vpc"
          EXISTING_VPC_ID=$(aws ec2 describe-vpcs \
            --filters "Name=tag:Name,Values=$VPC_NAME" \
            --query 'Vpcs[0].VpcId' \
            --output text \
            --region ${{ env.AWS_REGION }} 2>/dev/null || echo "")

          if [ "$EXISTING_VPC_ID" != "None" ] && [ -n "$EXISTING_VPC_ID" ]; then
            safe_import "aws_vpc.main" "$EXISTING_VPC_ID" "vpc" "VPC"
          fi

          # Import existing CloudFront Origin Access Control if it exists
          OAC_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-oac"
          EXISTING_OAC_ID=$(aws cloudfront list-origin-access-controls --query "OriginAccessControlList.Items[?Name=='$OAC_NAME'].Id" --output text 2>/dev/null || echo "")

          if [ "$EXISTING_OAC_ID" != "None" ] && [ -n "$EXISTING_OAC_ID" ]; then
            safe_import "module.s3_website.aws_cloudfront_origin_access_control.website" "$EXISTING_OAC_ID" "oac" "CloudFront Origin Access Control"
          fi

          # Note: Route53 zone is configured as a data source (data.aws_route53_zone.main) 
          # and doesn't need to be imported since it's not managed by Terraform
          echo "ℹ️ Route53 zone is configured as data source - no import needed"

          echo "✅ Resource import completed"

      - name: Terraform Apply with Robust Fallback Strategy
        run: |
          cd terraform/environments/${{ needs.detect-environment.outputs.main_env }}

          echo "🚀 Starting robust Terraform deployment strategy..."

          # Step 1: Validate state integrity
          echo "🔍 Step 1: Validating Terraform state integrity..."
          if ! terraform validate; then
            echo "❌ Terraform configuration validation failed"
            exit 1
          fi

          # Step 2: Refresh state to sync with actual AWS resources
          echo "🔄 Step 2: Refreshing Terraform state..."
          if terraform refresh -lock-timeout=300s; then
            echo "✅ State refresh completed successfully"
          else
            echo "⚠️ State refresh failed, attempting state unlock..."
            terraform force-unlock -force || echo "⚠️ Unable to unlock state"
            # Retry refresh after unlock
            terraform refresh -lock-timeout=300s || echo "⚠️ State refresh still failing"
          fi

          # Step 3: Create fresh plan with detailed output
          echo "📋 Step 3: Creating fresh Terraform plan..."
          terraform plan -out=tfplan -detailed-exitcode > plan.log 2>&1
          PLAN_EXIT_CODE=$?

          case $PLAN_EXIT_CODE in
            0)
              echo "✅ No changes required - infrastructure is up to date"
              ;;
            1)
              echo "❌ Terraform plan failed"
              cat plan.log
              exit 1
              ;;
            2)
              echo "📝 Changes detected - proceeding with apply"
              ;;
            *)
              echo "❌ Unexpected plan exit code: $PLAN_EXIT_CODE"
              cat plan.log
              exit 1
              ;;
          esac

          # Step 4: Apply with multiple fallback strategies
          if [ $PLAN_EXIT_CODE -eq 2 ]; then
            echo "🚀 Step 4: Applying Terraform configuration..."

            # First attempt: Full apply with timeout
            echo "🎯 Attempt 1: Full Terraform apply..."
            if timeout 1800 terraform apply -auto-approve -lock-timeout=300s tfplan; then
              echo "✅ Full Terraform apply completed successfully!"
            else
              echo "⚠️ Full apply failed, trying targeted deployment..."
              
              # Second attempt: Core infrastructure first
              echo "🏗️ Attempt 2: Core infrastructure deployment..."
              
              # Apply VPC and networking first
              terraform apply -auto-approve -lock-timeout=300s \
                -target=aws_vpc.main \
                -target=aws_subnet.private \
                -target=aws_internet_gateway.main \
                -target=aws_security_group.database \
                -target=aws_security_group.lambda || echo "⚠️ VPC deployment issues"
              
              # Apply secrets and database
              terraform apply -auto-approve -lock-timeout=300s \
                -target=aws_secretsmanager_secret.jwt_secret \
                -target=aws_secretsmanager_secret.database_password \
                -target=aws_secretsmanager_secret_version.jwt_secret \
                -target=aws_secretsmanager_secret_version.database_password \
                -target=aws_db_subnet_group.database \
                -target=aws_rds_cluster.database \
                -target=aws_rds_cluster_instance.database || echo "⚠️ Database deployment issues"
              
              # Apply S3 buckets
              terraform apply -auto-approve -lock-timeout=300s \
                -target=aws_s3_bucket.uploads \
                -target=module.s3_website \
                -target=module.lambda_backend.aws_s3_bucket.lambda_deployments || echo "⚠️ S3 deployment issues"
              
              # Apply serverless components
              terraform apply -auto-approve -lock-timeout=300s \
                -target=module.lambda_backend \
                -target=module.api_gateway || echo "⚠️ Serverless deployment issues"
              
              # Apply Route53 records
              terraform apply -auto-approve -lock-timeout=300s \
                -target=aws_route53_record.api \
                -target=aws_route53_record.website || echo "⚠️ DNS deployment issues"
              
              # Final attempt: Apply remaining resources
              echo "🔄 Final attempt: Applying any remaining resources..."
              terraform apply -auto-approve -lock-timeout=300s || echo "⚠️ Some resources may need manual intervention"
            fi
          fi

          # Step 5: Verify deployment and get outputs
          echo "🔍 Step 5: Verifying deployment..."
          if terraform output > outputs.log 2>&1; then
            echo "✅ Deployment outputs:"
            cat outputs.log
          else
            echo "⚠️ Could not retrieve all outputs:"
            cat outputs.log
          fi

          # Step 6: Validate critical resources
          echo "🔍 Step 6: Validating critical resources..."

          # Check if Lambda function exists
          LAMBDA_NAME=$(terraform output -raw lambda_function_name 2>/dev/null || echo "")
          if [ -n "$LAMBDA_NAME" ]; then
            if aws lambda get-function --function-name "$LAMBDA_NAME" >/dev/null 2>&1; then
              echo "✅ Lambda function '$LAMBDA_NAME' is accessible"
            else
              echo "⚠️ Lambda function '$LAMBDA_NAME' may not be ready yet"
            fi
          fi

          # Check if API Gateway exists
          API_URL=$(terraform output -raw api_url 2>/dev/null || echo "")
          if [ -n "$API_URL" ]; then
            echo "✅ API Gateway URL: $API_URL"
          else
            echo "⚠️ API Gateway URL not available in outputs"
          fi

          echo "🎉 Terraform deployment process completed!"

  # =================================
  # Deploy Frontend
  # =================================
  deploy-frontend:
    name: Deploy Frontend
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-infrastructure, build-frontend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download frontend artifacts
        uses: actions/download-artifact@v4
        with:
          name: frontend-${{ needs.detect-environment.outputs.environment }}-${{ needs.build-frontend.outputs.frontend-hash }}

      - name: Verify frontend artifacts
        run: |
          echo " Verifying frontend artifacts..."
          ls -la
          if [ -d "out" ]; then
            echo "✅ Frontend out directory found"
            ls -la out/
          else
            echo "❌ Frontend out directory not found, checking current directory"
            ls -la
            echo "📁 Creating minimal frontend structure..."
            mkdir -p out
            echo "<html><body><h1>Frontend Deployment</h1><p>Static files will be deployed here.</p></body></html>" > out/index.html
          fi

      - name: Deploy to S3
        run: |
          # Use dynamic S3 bucket name from environment detection
          echo "🚀 Deploying to S3 bucket: ${{ needs.detect-environment.outputs.s3_bucket }}"

          # Check if out directory exists
          if [ ! -d "out" ]; then
            echo "❌ out directory not found, creating minimal structure"
            mkdir -p out
            echo "<html><body><h1>Frontend Deployment</h1><p>Static files will be deployed here.</p></body></html>" > out/index.html
          fi

          # Deploy to S3
          aws s3 sync out/ s3://${{ needs.detect-environment.outputs.s3_bucket }} --delete

      - name: Invalidate CloudFront cache
        run: |
          # Use dynamic domain name from environment detection
          DOMAIN_NAME="${{ needs.detect-environment.outputs.environment }}.${{ needs.detect-environment.outputs.domain }}"
          echo " Invalidating CloudFront cache for domain: $DOMAIN_NAME"

          DISTRIBUTION_ID=$(aws cloudfront list-distributions --query "DistributionList.Items[?Aliases.Items[?contains(@, '$DOMAIN_NAME')]].Id" --output text)
          if [ "$DISTRIBUTION_ID" != "None" ] && [ "$DISTRIBUTION_ID" != "" ]; then
            echo " Invalidating CloudFront cache for distribution: $DISTRIBUTION_ID"
            aws cloudfront create-invalidation --distribution-id $DISTRIBUTION_ID --paths "/*"
          else
            echo "⚠️ No CloudFront distribution found for domain: $DOMAIN_NAME"
          fi

  # =================================
  # Deploy Backend
  # =================================
  deploy-backend:
    name: Deploy Backend
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-infrastructure, build-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Download backend artifacts
        uses: actions/download-artifact@v4
        with:
          name: backend-${{ needs.detect-environment.outputs.environment }}-${{ needs.build-backend.outputs.backend-hash }}

      - name: Create Lambda deployment package
        run: |
          # Create deployment directory
          mkdir -p lambda-deployment
          cd lambda-deployment

          # Copy built backend files from downloaded artifacts (exclude lambda-deployment directory)
          echo "📦 Copying backend files from artifacts..."
          cp -r ../* . 2>/dev/null || true

          # Remove the lambda-deployment directory if it was copied
          rm -rf ./lambda-deployment

          # Create node_modules directory structure
          mkdir -p node_modules/@digital-persona

          # Copy workspace package dist folders and package.json files
          echo "📦 Copying workspace packages..."
          if [ -d "../../packages/shared/dist" ]; then
            cp -r ../../packages/shared/dist ./node_modules/@digital-persona/shared/
            cp ../../packages/shared/package.json ./node_modules/@digital-persona/shared/
          else
            echo "⚠️ Shared package dist not found, creating placeholder"
            mkdir -p ./node_modules/@digital-persona/shared/
            echo '{"name":"@digital-persona/shared","version":"1.0.0"}' > ./node_modules/@digital-persona/shared/package.json
          fi

          if [ -d "../../packages/database/dist" ]; then
            cp -r ../../packages/database/dist ./node_modules/@digital-persona/database/
            cp ../../packages/database/package.json ./node_modules/@digital-persona/database/
          else
            echo "⚠️ Database package dist not found, creating placeholder"
            mkdir -p ./node_modules/@digital-persona/database/
            echo '{"name":"@digital-persona/database","version":"1.0.0"}' > ./node_modules/@digital-persona/database/package.json
          fi

          # Copy production dependencies if they exist
          echo " Copying production dependencies..."
          if [ -d "../../node_modules" ]; then
            cp -r ../../node_modules/* ./node_modules/
          else
            echo "⚠️ node_modules not found, creating minimal package.json"
            echo '{"name":"lambda-deployment","version":"1.0.0","dependencies":{}}' > package.json
          fi

          # Clean up unnecessary files
          echo "🧹 Cleaning up deployment package..."
          find . -name "*.d.ts" -delete
          find . -name "*.d.mts" -delete
          find . -name "*.d.cts" -delete
          find . -name "*.map" -delete
          find . -name "*.test.js" -delete
          find . -name "*.spec.js" -delete
          find . -name "__tests__" -type d -exec rm -rf {} + 2>/dev/null || true

          # Create deployment zip
          echo "📦 Creating deployment zip..."
          zip -r ../lambda-deployment.zip . -x "*.git*" "*.DS_Store*"

          cd ..

          echo "✅ Lambda deployment package created successfully"

      - name: Deploy Lambda function
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"

          echo "🔄 Updating Lambda function: $FUNCTION_NAME"

          # Wait for Lambda to be ready
          check_lambda_ready() {
            local status=$(aws lambda get-function --function-name $FUNCTION_NAME --query 'Configuration.State' --output text 2>/dev/null)
            echo "Lambda status: $status"
            [[ "$status" == "Active" ]]
          }

          # Wait for Lambda to be ready before updating
          echo "⏳ Waiting for Lambda to be ready..."
          for i in {1..30}; do
            if check_lambda_ready; then
              echo "✅ Lambda is ready for update"
              break
            fi
            echo "⏳ Waiting... (attempt $i/30)"
            sleep 10
          done

          # Update Lambda with retry logic
          update_lambda() {
            local max_attempts=5
            local attempt=1
            
            while [ $attempt -le $max_attempts ]; do
              echo "🔄 Attempting Lambda update (attempt $attempt/$max_attempts)..."
              
              if aws lambda update-function-code --function-name $FUNCTION_NAME --zip-file fileb://lambda-deployment.zip; then
                echo "✅ Lambda update successful"
                return 0
              else
                echo "❌ Lambda update failed (attempt $attempt/$max_attempts)"
                
                if [ $attempt -lt $max_attempts ]; then
                  echo "⏳ Waiting before retry..."
                  sleep $((attempt * 10))
                fi
                
                attempt=$((attempt + 1))
              fi
            done
            
            echo "❌ All Lambda update attempts failed"
            return 1
          }

          update_lambda

      - name: Wait for Lambda update
        run: |
          # Use dynamic function name from environment detection
          FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
          aws lambda wait function-updated --function-name $FUNCTION_NAME

  # =================================
  # Health Check
  # =================================
  health-check:
    name: Health Check
    runs-on: ubuntu-latest
    needs: [detect-environment, deploy-frontend, deploy-backend]
    if: ${{ needs.detect-environment.outputs.should_deploy == 'true' || github.event_name == 'workflow_dispatch' }}

    steps:
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Get API Gateway URL
        id: api-url
        run: |
          # Use dynamic environment variables
          ENVIRONMENT="${{ needs.detect-environment.outputs.environment }}"
          MAIN_ENV="${{ needs.detect-environment.outputs.main_env }}"
          PROJECT_NAME="dpp"

          # Primary expected API Gateway name (matches module configuration)
          PRIMARY_API_NAME="${MAIN_ENV}-${ENVIRONMENT}-${PROJECT_NAME}-api"
          # Alternative naming patterns to check
          ALT_API_NAME1="${{ needs.detect-environment.outputs.function_name }}"
          ALT_API_NAME2="${ENVIRONMENT}-${PROJECT_NAME}-api"

          echo "🔍 Looking for API Gateway with multiple naming patterns:"
          echo "  Primary: $PRIMARY_API_NAME"
          echo "  Alternative 1: $ALT_API_NAME1"
          echo "  Alternative 2: $ALT_API_NAME2"

          # Function to find API by name
          find_api_by_name() {
            local api_name="$1"
            aws apigatewayv2 get-apis --query "Items[?Name=='$api_name'].ApiId" --output text | tr -d '[:space:]'
          }

          # Try to find API Gateway with different naming patterns
          API_ID=""
          for api_name in "$PRIMARY_API_NAME" "$ALT_API_NAME1" "$ALT_API_NAME2"; do
            echo "🔍 Checking for API Gateway: $api_name"
            API_ID=$(find_api_by_name "$api_name")
            if [ -n "$API_ID" ] && [ "$API_ID" != "None" ] && [ "$API_ID" != "" ]; then
              echo "✅ Found API Gateway '$api_name' with ID: $API_ID"
              break
            fi
          done

          # If still not found, list all APIs for debugging
          if [ -z "$API_ID" ] || [ "$API_ID" = "None" ] || [ "$API_ID" = "" ]; then
            echo "❌ No API Gateway found with expected names"
            echo "Available APIs in region ${{ env.AWS_REGION }}:"
            aws apigatewayv2 get-apis --query 'Items[*].{Name:Name,Id:ApiId,Protocol:ProtocolType}' --output table
            
            # Try to find any API that contains our project name
            echo "🔍 Searching for APIs containing '$PROJECT_NAME'..."
            FUZZY_API_ID=$(aws apigatewayv2 get-apis --query "Items[?contains(Name, '$PROJECT_NAME')].ApiId" --output text | head -n1 | tr -d '[:space:]')
            
            if [ -n "$FUZZY_API_ID" ] && [ "$FUZZY_API_ID" != "None" ]; then
              echo "⚠️ Found API containing '$PROJECT_NAME': $FUZZY_API_ID"
              API_ID="$FUZZY_API_ID"
            else
              echo "❌ No API Gateway found containing '$PROJECT_NAME'"
              exit 1
            fi
          fi

          # Construct the API URL
          API_URL="https://${API_ID}.execute-api.${{ env.AWS_REGION }}.amazonaws.com/v1"

          echo "✅ Using API Gateway: $API_ID"
          echo "🌐 API URL: $API_URL"

          # Set the output
          echo "api_url=${API_URL}" >> $GITHUB_OUTPUT

      - name: Test API health endpoint
        run: |
          # Use the dynamically generated API URL from the previous step
          API_URL="${{ steps.api-url.outputs.api_url }}/health"

          # Clean the URL to remove any whitespace or special characters
          API_URL=$(echo "$API_URL" | tr -d '[:space:]')

          echo "Testing API health endpoint: $API_URL"
          echo "URL length: ${#API_URL} characters"

          # Test with detailed error capture
          for attempt in {1..5}; do
            echo "🔄 Health check attempt $attempt/5..."
            
            # Capture full response
            RESPONSE=$(curl -s -w "\nHTTP_STATUS:%{http_code}\nTIME:%{time_total}" "$API_URL" 2>&1)
            HTTP_STATUS=$(echo "$RESPONSE" | grep "HTTP_STATUS:" | cut -d: -f2)
            RESPONSE_TIME=$(echo "$RESPONSE" | grep "TIME:" | cut -d: -f2)
            RESPONSE_BODY=$(echo "$RESPONSE" | sed '/HTTP_STATUS:/d' | sed '/TIME:/d')
            
            echo "Status: $HTTP_STATUS"
            echo "Response Time: $RESPONSE_TIME seconds"
            echo "Response Body: $RESPONSE_BODY"
            
            if [ "$HTTP_STATUS" = "200" ]; then
              echo "✅ Health check passed!"
              break
            else
              echo "❌ Health check failed (attempt $attempt/5)"
              
              if [ $attempt -lt 5 ]; then
                echo "⏳ Waiting 30 seconds before retry..."
                sleep 30
              else
                echo "❌ All health check attempts failed"
                
                # Capture CloudWatch logs for debugging
                echo "📋 Capturing CloudWatch logs for debugging..."
                FUNCTION_NAME="${{ needs.detect-environment.outputs.function_name }}"
                LOG_GROUP_NAME="${{ needs.detect-environment.outputs.log_group_name }}"
                
                # Get the latest log stream
                LOG_STREAM=$(aws logs describe-log-streams \
                  --log-group-name "$LOG_GROUP_NAME" \
                  --order-by LastEventTime \
                  --descending \
                  --max-items 1 \
                  --query 'logStreams[0].logStreamName' \
                  --output text 2>/dev/null || echo "No log streams found")
                
                if [ "$LOG_STREAM" != "None" ] && [ "$LOG_STREAM" != "" ]; then
                  echo " Latest log stream: $LOG_STREAM"
                  aws logs get-log-events \
                    --log-group-name "$LOG_GROUP_NAME" \
                    --log-stream-name "$LOG_STREAM" \
                    --start-time $(date -d '10 minutes ago' +%s)000 \
                    --query 'events[*].message' \
                    --output text
                fi
                
                exit 1
              fi
            fi
          done
